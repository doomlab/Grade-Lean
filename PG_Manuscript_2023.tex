% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  man]{apa7}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

\makeatletter
\usepackage{etoolbox}
\patchcmd{\maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\keywords{reliability, teaching effectiveness, fairness, grading, evaluations}
\DeclareDelayedFloatFlavor{ThreePartTable}{table}
\DeclareDelayedFloatFlavor{lltable}{table}
\DeclareDelayedFloatFlavor*{longtable}{table}
\makeatletter
\renewcommand{\efloat@iwrite}[1]{\immediate\expandafter\protected@write\csname efloat@post#1\endcsname{}}
\makeatother
\usepackage{lineno}

\linenumbers
\usepackage{csquotes}
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={The Reliability of Instructor Evaluations},
  pdfauthor={Erin M. Buchanan1, Jacob Miranda2, \& Christian Stephens2},
  pdflang={en-EN},
  pdfkeywords={reliability, teaching effectiveness, fairness, grading, evaluations},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{The Reliability of Instructor Evaluations}
\author{Erin M. Buchanan\textsuperscript{1}, Jacob Miranda\textsuperscript{2}, \& Christian Stephens\textsuperscript{2}}
\date{}


\shorttitle{RELIABILITY EVALUATIONS}

\authornote{

The authors made the following contributions. Erin M. Buchanan: Conceptualization, Writing - Original Draft Preparation, Writing - Review \& Editing, Analysis; Jacob Miranda: Writing - Original Draft Preparation; Christian Stephens: Writing - Original Draft Preparation.

Correspondence concerning this article should be addressed to Erin M. Buchanan, 326 Market St., Harrisburg, PA 17010. E-mail: \href{mailto:ebuchanan@harrisburgu.edu}{\nolinkurl{ebuchanan@harrisburgu.edu}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Harrisburg University of Science and Technology\\\textsuperscript{2} University of Alabama}

\abstract{%
Student evaluations of teaching are regularly used within college classroom to gauge effectiveness of instruction, provide evidence for administrative decision making, and inform instructors of course feedback. The validity of teaching evaluations is often questioned, as they appear to be influenced by outside of teaching factors such as gender, race/ethnicity, grading, previous student achievement, and more. However, teaching evaluations do appear to be a reliable measure, often showing strong correlations for an instructor. In this study, we investigate over 30 years of teaching evaluations to determine the reliability of teaching evaluations across course, instructor, and time. Generally, instructors teaching the same course within the same semester showed the highest reliability estimates, with lower estimates for the same course in different semesters. The reliability of instructor's evaluations showed a small decrease over time. Finally, we investigated the impact of a validity measurement (perceived fairness) on reliability and found no evidence that this variable influence reliability estimates.
}



\begin{document}
\maketitle

In the United States, college and university professors are evaluated to
varying degrees on research productivity, service, and teaching
effectiveness. These dimensions are often used for high-stakes
administration decisions (e.g., hiring, retention, promotion, pay, and
tenure, Freishtat, 2014; Hornstein, 2017; Spooren et al., 2013) !!stoebe, 2020!!.
Depending on the institution, a major failure of one these areas could
jeopardize a professors' position within the department; thus,
evaluating research, service, and teaching is of the utmost importance.
Focusing on evaluating educators on teaching effectiveness, however, is
both difficult and costly. Indeed, the vast majority of the 9,000
professors polled by the American Association of University Professors
shared that teaching needs to be taken as seriously as research and
service (!!Flaherty, 2015!!). As students consider rising tutition
costs, perceived quality education can improve student engagement and
retention.

Teaching effectiveness can be defined as the degree to which student
achievement is facilitated {[}i.e., how much have students learned in a
particular course; P. A. Cohen (1981){]}. Generally, the assessment of teaching
effectiveness comes from students and their evaluations which may focus
on the instructor or the course specifically {[}e.g., ``Student Opinion of
Instruction'', ``Student Evaluations of Teaching'', ``Students Opinion of
Teaching Effectiveness'', ``Students Evaluation of Faculty'', ``Overall
Course Ratings'', ``Instruction Rating''; P. A. Cohen (1981){]}. !!Flaherty, 2020!!
Often these are described as ``quality'' of an individual course
(Gillmore et al., 1978; Marsh, 2007). Teaching effectiveness measures are
designed to tap into factors of teaching, such as communication,
organization, instructor behavior, grading, and more (Hattie \& Marsh, 1996).
Given teaching evaluations use in administrative decisions, both
reliability and validity should be demonstrated for the measurement to
have utility. Therefore, the natural question arises: are students'
evaluation of the course and/or instructor reliable and valid measures
of teaching effectiveness?

\hypertarget{validity}{%
\subsection{Validity}\label{validity}}

Sheehan (1975)'s review of the literature nearly 50 years ago indicated
multiple factors of bias that likely exist within student evaluations:
1) student demographics: gender, class, age, previous achievement, 2)
class type: subject matter, size, degree requirements, and 3)
instructor: gender, rank, gender-match to student. Even now, these
concerns remain (Boring et al., 2016; Hornstein, 2017; Uttl et al., 2017) !! dunn et al.,
2016!!. P. A. Cohen (1981)'s early work on the relationship between student
achievement and instruction rating indicated a potential moderate
relationship; however, recent meta-analyses demonstrate that student
evaluations of teaching are likely unrelated to learning (Uttl et al., 2017).
Boring et al. (2016) estimate that the bias in student evaluations are unable to
be fixed due to the complex interaction of factors within evaluations.

Systemic reviews and recent studies underscore that sexism
(MacNell et al., 2015; Mitchell \& Martin, 2018), racism (Smith \& Hawkins, 2011), and general bias
pervades students' evaluations of traditional courses and possibly exist
for online ones as well (Heffernan, 2022; Rovai et al., 2006; Zheng et al., 2023) !!
Sullivan et al., 2013 !!. Individual factors may also yield some
influence, including instructors' cultural background (Fan et al., 2019),
attractiveness (Felton et al., 2008) !!Wright, 2008!!, position ranking
(Johnson et al., 2013), and students' expected grade from the course (Chen et al., 2017; Crumbley et al., 2001; Marks, 2000). Others suggest biasing factors of students'
ratings include the volume of the instructor's voice and how legible
their instructor's writing is {[}!! Becker et al., 2012 !!{]}. !!Stroebe
(2018)!! underscores the possible danger of an incentive system that is
tied to student ratings; instructors may be then incentivized to be a
less effective teacher (e.g., grade leniently, choose to teach courses
based off student interest) rather than challenge students critically.

One of the most commonly proposed solutions is to use multiple
evaluations of teaching effectiveness {[}e.g., subject-matter sit-ins on
lecture, peer reviews of course curriculum (Benton \& Young, 2018; Berk, 2018; Esarey \& Valdes, 2020; Kornell \& Hausman, 2016). However, the cost of implementing a more
accurate multi-pronged approach may be more than universities can
afford, especially given tight budgets and current instructor
expectations. The current zeitgeist is often to continue using student
evaluations of teaching as the most affordable solution in terms of both
time and money. Students' ratings may show some utility at indicating to
other students which classes to pursue and with whom {[}!! Stankiewicz,
2015 !!{]}, and unfortunately, even if instructors believe such ratings to
be an inappropriate, it may influence their self-efficacy as an educator
regardless (Boswell, 2016). While student evaluations are often
considered non-valid measurements of teaching effectiveness, others
argue that calls for the complete removal students' voices from the
process is potentially the wrong course of action (Benton \& Ryalls, 2016).

\hypertarget{perceived-fairness}{%
\subsection{Perceived fairness}\label{perceived-fairness}}

Our study focused on potential sources of validity bias using ratings of
grading within the course (which will be called perceived fairness).
Extant research tends to confirm that instructor evaluations are
influenced by students' grades, possibly pressuring some instructors
into reducing the rigor of their course for the sake of attaining higher
evaluation ratings (Greenwald \& Gillmore, 1997; Marks, 2000). However, as pointed out
by !!Wright (2008)!!, students' expectations of their final grades may
not affect ratings nearly as much as their perceived fairness of the
grading process. Professors who are consistent, representative,
accurate, unbiased, and correctable in their grading may receive high
evaluation ratings regardless of how much a student learns or what
his/her final grade turns out to be (Horan et al., 2010; Leventhal, 1980). Thus,
grades may predict evaluation ratings only so much as students perceive
their grade and the processes by which they were determined as fair
(Tata, 1999).

Additionally, the different facets leading into a final grade's
calculation may play on each other as students consider fairness in
their evaluations. For example, Tripp et al. (2019) found that students'
perceived fairness of their instructors' grading processes affected
their perceived fairness of their assigned grade, which then translated
to their evaluation ratings of teacher effectiveness. Perceived fairness
of the course workload and difficulty may also be inversely related to
perceived fairness of the grading process as a challenging professor may
be thought of as less fair (Marks, 2000). Access to grading criteria,
frequency of feedback, and proactive instruction are other aspects of
grading known to explicitly affect perceived fairness (Pepper \& Pathak, 2008); in
turn, the fairness of these aspects must be factored in as well. Taken
together, students' perceived fairness of grading may be more akin to
comprehensive assessments of the instructor rather than face-value
judgments of their grade.

\hypertarget{reliability}{%
\subsection{Reliability}\label{reliability}}

Past investigations utilizing large samples concluded student ratings
are reliable and stable (Arubayi, 1987; Marsh \& Roche, 1997). More recently, a
review found that students' ratings within the same class tend to be
internally consistent when teaching effectiveness was assessed through
several items, reliable across students within the same class, and
reliable across the same instructor across multiple courses
(Benton \& Cashin, 2014). Students who rated a retrospectively rated a course one
to three years after the course showed high correlations with their
previous course ratings (Overall \& Marsh, 1980). Results from studies that tease
apart variance in ratings due to instructor, course, and student factors
indicate that each is an essential source of variance, which can
influence the reliability of instruction evaluation (Feistauer \& Richter, 2017). In
general, research appears to support the reliability of student
evaluations of teaching, yet, only a few studies have examined this
reliability across instructor, course, and time. Research into teaching
effectiveness appears to suggest that instructors have stable
evaluations over time (Marsh, 2007), and our study extends this work to
examine reliability patterns over 30 years of evaluations.

\hypertarget{the-current-study}{%
\subsection{The current study}\label{the-current-study}}

The current study is similar in scope to recent work (Boring et al., 2016; Fan et al., 2019) in its calibration of teacher evaluations collected over an
extensive period. Boring et al. (2016)`s investigation on both French instructors
and U.S. teaching assistants' gender ranged across five years;
similarly, Fan et al. (2019)`s investigated the topic across seven. Their
utilization of multi-sections has been described as the gold standard
for researching students' ratings. Thus, we aimed to follow their lead
by analyzing the reliability of students' ratings provided the same or
different instructor, course type, and/or semester in addition to
testing reliability over more than 30 years of data. We examined the
impact of a potential validity variable on the reliability of ratings
using perceived fairness of grading. Therefore, we sought to explore the
following research questions:

Exploratory Research Questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  What is the reliability of instructor evaluations?
\item
  Are instructor evaluations reliable across time?
\item
  Is the average level of perceived fairness of the grading in the
  course a moderator of reliability in instructor evaluations?
\item
  Does the average variability in instructor fairness rating moderate
  reliability of instructor evaluations?
\end{enumerate}

The following was pre-registered as a secondary data analysis at:
\url{https://osf.io/czb4f}. The manuscript, code, and data can be found on
our Open Science Framework page at: \url{https://osf.io/k7zh2/} or GitHub:
\url{https://github.com/doomlab/Grade-Lean}. This manuscript was written
with the \emph{R} packages \emph{papaja} (Aust et al., 2022), \emph{rio} (Becker et al., 2021), \emph{dplyr}
(Wickham et al., 2020), \emph{nlme} (Pinheiro et al., 2017), \emph{ggplot2} (Wickham, 2016), \emph{MuMIn}
(Bartoń, 2020), \emph{ppcor} (Kim, 2015), and \emph{effectsize} (\emph{Effectsize}, 2023).

\hypertarget{method}{%
\section{Method}\label{method}}

\hypertarget{data-source}{%
\subsection{Data Source}\label{data-source}}

The archival study was conducted using data from the psychology
department at a large Midwestern public university. We used data from
2898
undergraduate, 274
mixed-level undergraduate, and
42 graduate
psychology classes taught from 1987 to 2018 that were evaluated by
students using the same 15-item instrument. Faculty followed set
procedures in distributing scan forms no more than two weeks before the
conclusion of the semester. A student was assigned to collect the forms
and deliver them to the departmental secretary. The instructor was
required to leave the room while students completed the forms. In the
last several years of evaluations, online versions of these forms were
used with faculty encouraged to give students time to complete them in
class while they were outside the classroom.

The questionnaire given to students can be found at
\url{https://osf.io/4sphx}. These items were presented with a five-point
scale from 1 (\emph{strongly disagree}) to 5 (\emph{strongly agree}). For this
study, the overall instructor evaluation question was ``The overall
quality of this course was among the top 20\% of those I have taken.''.
For fairness, we used the question of ``The instructor used fair and
appropriate methods in the determination of grades.''. The ratings were
averaged for each course, and the sample size for each rating was
included.

\hypertarget{planned-analyses}{%
\subsection{Planned Analyses}\label{planned-analyses}}

The evaluations were filtered for those with at least fifteen student
ratings for the course (Rantanen, 2012). We performed a robustness check
for the first research question on the data when the sample size is at
least \emph{n} = 10 up to \emph{n} = 14 (i.e., on all evaluations with at least 10
ratings, then at least 11 ratings, etc.) to determine if the reliability
estimates are stable at lower sample sizes. We first screened the
dataset (two evaluation questions, sample size for course) for accuracy
errors, linearity, normality, and homoscedasticity. The data is assumed
to not have traditional ``outliers'', as these evaluations represent true
averages from student evaluations. If the linearity assumption fails, we
considered potential nonparametric models to address non-linearity.
Deviations from normality were noted as the large sample size should
provide robustness for any violations of normality. If data appears to
be heteroscedastic, we used bootstrapping to provide estimates and
confidence intervals.

This data was considered structured by instructor; therefore, all
analyses below were coded in \emph{R} using the \emph{nlme} package
(Pinheiro et al., 2017) to control for correlated error of instructor as a
random intercept in a multilevel model. Multilevel models allow for
analysis of repeated measures data without collapsing by participant
{[}i.e., each instructor/semester/course combination can be kept separate
without averaging over these measurements; Gelman (2006){]}. Random
intercept models are regression models on repeated data that structure
the data by a specified variable, which was instructor in this analysis.
Therefore, each instructor's average rating score was allowed to vary
within the analysis, as ratings would be expected to be different from
instructor to instructor. In each of the analyses described below, the
number of students providing ratings for the course was included as a
control variable to even out differences in course size as an influence
in the results. However, this variable was excluded if the models did
not converge. The dependent variable and predictors varied based on the
research question, and these are described with each analysis below.

\hypertarget{rq-1}{%
\subsubsection{RQ 1}\label{rq-1}}

In this research question, we examined the reliability of instructor
evaluations on the overall rating and separately on the fairness rating.
We calculated eight types of reliability using course (same or
different) by instructor (same or different) by semester (same or
different). The dependent variable was the first question average with a
predictor of the comparison question average, and both sample sizes
(first sample size, comparison sample size). Instructor code was used as
the random intercept for both ratings (i.e., two instructor random
intercepts, first and comparison). The value of interest was the
standardized regression coefficient for the fixed effect of question
from this model. Given that the large sample size will likely produce
``significant'' \emph{p}-values, we used the 95\% CI to determine which
reliability values were larger than zero and to compare reliability
estimates to each other.

\hypertarget{rq-2}{%
\subsubsection{RQ 2}\label{rq-2}}

We used the reliability for the same instructor and course calculated as
described in RQ1 at each time point difference between semesters. For
example, the same semester would create a time difference of 0. The next
semester (Spring to Summer, Summer to Fall, Fall to Spring) would create
a time difference of 1. We used the time difference as a fixed effect to
predict reliability for the overall question only with a random
intercept of instructor. We used the coefficient of time difference and
its confidence interval to determine if there was a linear change over
time. Finally, we plotted the changes over time to examine if this
effect was non-linear in nature and discuss implications of the graph.

\hypertarget{rq-3}{%
\subsubsection{RQ 3}\label{rq-3}}

Using the reliability estimates from RQ 2, we then added the average
rating for the fairness question as the moderator with time to predict
reliability. Fairness was calculated as the average of the fairness
question for all courses involved in the reliability calculation for
that instructor and time difference. Therefore, this rating represented
the average perceived fairness of grading at the time of ratings. If
this interaction effect's coefficient does not include zero, we
performed a simple slopes analysis to examine the effects of instructors
who were rated at average fairness, one standard deviation below
average, and one standard deviation above average (J. Cohen et al., 2003).

\hypertarget{rq-4}{%
\subsubsection{RQ 4}\label{rq-4}}

Finally, we examined the average standard deviation of fairness ratings
as a moderator of with time to predict reliability. This variable
represented the variability in perceived fairness in grading from
student evaluations, where small numbers indicated relative agreement on
the rating of fairness and larger values indicated a wide range of
fairness ratings. The variability in fairness ratings was calculated in
the same way as the mean fairness, which was only for the instructor and
semester time difference evaluations that were used to calculate the
reliability estimate. This research question was assessed the same way
as research question three.

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{data-screening}{%
\subsection{Data Screening}\label{data-screening}}

The overall dataset was screened for normality, linearity, homogeneity,
and homoscedasticity using procedures from Tabachnick et al. (2019). Data
generally met assumptions with a slight skew and some heterogeneity. The
complete anonymized dataset and other information can be found online at
\url{https://osf.io/k7zh2}. This page also includes the manuscript written
inline with the statistical analysis with the \emph{papaja} package
(Aust et al., 2022) for interested researchers/reviewers who wish to recreate
these analyses. The bootstrapped versions of analyses and robustness
analysis can be found online on our OSF page with a summary of results.
We originally planned to bootstrap all analyses; however, the compute
time for research question 1 was extremely long due to the size and
complexity of the multilevel models, and therefore, we did not bootstrap
that research question.

\hypertarget{descriptive-statistics}{%
\subsection{Descriptive Statistics}\label{descriptive-statistics}}

3214 evaluations included at least 15 student evaluations
for analysis. Table \ref{tab:table1} portrays the descriptive
statistics for each course level including the total number of
evaluations, unique instructors, unique course numbers, and average
scores for the two rating items. Students additionally projected their
course grade for each class (\emph{A} = 5, \emph{B} = 4, \emph{C} = 3, \emph{D} = 2, \emph{F} =
1), and the average for this item is included for reference. Overall,
231 unique instructors and
70 unique courses were included in
the analyses below across 94
semesters.

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:table1}Descriptive Statistics of Included Courses}

\begin{tabular}{llll}
\toprule
Statistic & Undergraduate & Mixed & Master's\\
\midrule
N Total & 2898 & 274 & \ \ 42\\
N Instructors & 223 & 40 & 10\\
N Courses & 41 & 21 & 8\\
Average N Ratings & 34.39 & 21.15 & 21.10\\
Average Overall & 3.94 & 4.01 & 3.72\\
SD Overall & 0.55 & 0.59 & 0.67\\
Average Fairness & 4.46 & 4.50 & 4.19\\
SD Fairness & 0.35 & 0.38 & 0.55\\
Average Grade & 4.26 & 4.52 & 4.41\\
SD Grade & 0.33 & 0.27 & 0.34\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\hypertarget{rq-1-1}{%
\subsection{RQ 1}\label{rq-1-1}}

Each individual evaluation was compared to every other evaluation
resulting in 5163291 total comparisons. Eight combinations of
ratings were examined using instructor (same, different), course (same,
different), and semester (same, different) on both the overall and
fairness evaluation ratings separately. One of the individual ratings
was used to predict the comparison rating (i.e., question 1 was used to
predict a comparison question 1 for the same instructor, different
instructor, same semester, different semester, etc.), and the number of
ratings (i.e., rating sample size) per question were used as
fixed-effects covariates. The instructor(s) were used as a random
intercept to control for correlated error and overall average rating per
instructor. The effects were then standardized using the \emph{parameters}
package (Lüdecke et al., 2023). The data was sorted by year and semester such
that ``predictor'' was always an earlier semester predicting a later
semester's scores, except in cases of the the same semester comparisons.
Therefore, positive standardized scores indicate that scores tend to go
up over time, while negative scores indicate that scores tend to go down
over time.

As shown in \ref{fig:figure1}, reliability was highest when calculated
on the same instructor in the same semester and within the same course
for both overall rating and fairness. This reliability was followed by
the same instructor, same semester, and different courses. Next, the
reliability for same instructor, same course, and different semesters
was greater than zero and usually overlapped in confidence interval with
same instructor, same semester, and different courses. Interestingly,
the same instructor with different courses and semesters showed a
non-zero negative relationship, indicating that ratings generally were
lower for later semesters in different courses.

For different instructors, we found positive non-zero readabilities when
they were at least calculated on the same semester or course. These
values were very close to zero, generally in the .01 to .05 range. Last,
the reliabilities that were calculated on different courses, semesters,
and instructors include zero in their confidence intervals. Exact values
can be found in the online supplemental document with the robustness
analysis in csv format. Robustness analyses revealed the same pattern
and strength of results for evaluation reliabilities when sample size
for evaluations was considered at \emph{n} = 10, 11, 12, 13, and 14.

\begin{figure}
\centering
\includegraphics{PG_Manuscript_2023_files/figure-latex/figure1-1.pdf}
\caption{\label{fig:figure1}Reliability estimates for instructor, course, and semester combinations.}
\end{figure}

\hypertarget{rq-2-1}{%
\subsection{RQ 2}\label{rq-2-1}}

The paired evaluations were then filtered to only examine course and
instructor matches to explore the relation of reliability across time.
Reliability was calculated by calculating the partial correlation
between the overall rating for the course first evaluation and the
overall rating for the course second evaluation, controlling for the
number of ratings within those average scores. This reliability was
calculated separately for each instructor and semester difference (i.e.,
the time between evaluations, 0 means same semester, 1 means the next
semester, 2 means two semesters later, etc.). The ratings were filtered
so that at least 10 pairs of ratings were present for each instructor
and semester difference combination (Weaver \& Koopman, 2014). Of
36084 possible matched instructor and course pairings,
30728 included at
least 10 pairings, which was 1009 total instructor and
semester combinations.

The confidence interval for the effect of semester difference predicting
reliability did not cross zero, \emph{b} =
-0.004, 95\% CI
{[}-0.005,
-0.003{]}, \(R^2\) =
.04.
The coefficient, while small, represents a small effect of time on the
reliability of instructor ratings. As shown in \ref{fig:figure2},
reliability appears to decrease across time.

\begin{figure}
\centering
\includegraphics{PG_Manuscript_2023_files/figure-latex/figure2-1.pdf}
\caption{\label{fig:figure2}Reliability estimates for same instructor and course across time.}
\end{figure}

\hypertarget{rq-3-1}{%
\subsection{RQ 3}\label{rq-3-1}}

The confidence interval for the interaction of semester time difference
and average fairness did cross zero, \emph{b} =
-0.001, 95\% CI
{[}-0.007,
0.005{]}, \(R^2\) =
.04.
Therefore, there was no effect of the interaction of average fairness
with semester differences in predicting reliability. Similarly, average
fairness did not predict reliability overall, \emph{b} =
-0.041, 95\% CI
{[}-0.226,
0.143{]}.

\hypertarget{rq-4-1}{%
\subsection{RQ 4}\label{rq-4-1}}

The confidence interval for the interaction of variability of fairness
and semester time difference did cross zero, \emph{b} =
-0.010, 95\% CI
{[}-0.022,
0.002{]}, \(R^2\) =
.05.
The variability of fairness also did not predict reliability overall,
\emph{b} = 0.291, 95\% CI
{[}-0.091,
0.672{]}.

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

This investigation measured student evaluation of teaching's reliability
by calculating the reliability of evaluations across instructors,
semesters, and courses. In our first question, we showed that
evaluations of the same instructor within the same course and same
semester were the most reliable, followed by different courses and
different semesters. We extended previous meta-analyses on reliability
to show that reliability appears to slightly, but significantly,
decrease over time --- a new finding in comparison to the work of
Marsh (2007). Last, we explored the relationship of a variable that
potentially impacts the validity of student evaluations of teaching:
perceived fairness in grading. Perceived fairness did not appear to
impact reliability scores, nor did it interact with time to predict
reliability scores. While variability in perceived fairness is found
across and within instructor ratings, this variability also did not
impact reliability information.

This study extends previous work with several new strengths (Benton \& Cashin, 2014; Benton \& Ryalls, 2016; Marsh, 2007; Zhao \& Gallant, 2011). The data included in this
manuscript represents over 30 years of teaching evaluations and was
analyzed for reliability within and across courses, semesters, and
instructors; thus, providing new insights into the expected level of
reliability in different calculation scenarios. Sensitivity and
bootstrapped analyses show that these results are robust even with a
smaller number of evaluations used, supporting and extending work by
Rantanen (2012). Last, we investigated the impact of validity variables on
reliability, not just the overall validity of evaluations based on
various potential biases.

Given these results, what should instructors and administrators do with
student evaluations of teaching? Benton and Young (2018) provide a comprehensive
checklist of ways to assess teaching and interpret evaluations in light
of the long history of validity questions for student evaluations of
teaching. Here, we add that it is important to understand that
reliability will vary by course and semester as instructor variability
is usually expected. It is tempting to think that the same instructor
teaching the same course should reliably get the same evaluations;
however, we should consider that instructors will grow and change over
time, which may contribute to lessened reliability across time (in
addition to other known biases, such as age). Further, facets of the
different courses taught likely contribute to the lessened reliability
between courses taught by the same instructor (i.e., required statistics
courses versus elective courses). As Benton and Young (2018) describes, the
evaluation procedure should be useful, and it may not be fruitful to
compare different years or even courses, and evaluations should be
contextualized to the course and semester they were received in.

While this study provides valuable evidence about evaluation
reliability, the study only includes one department of evaluation
scores, and the descriptive statistics suggest these evaluations are
often at ceiling on a 1 to 5 Likert type scale. Evaluations are always
biased by the students who are in class or fill out the online survey
--- information about missing student perceptions are never recorded.
The concerns about the validity of evaluations are still relevant, and
it may be that reliability is interesting but not altogether useful if
the scores are not valid representations of teaching effectiveness. As
universities struggle to balance demands of higher education cost and
student enrollment, teaching effectiveness may be a critical target for
administrators to ensure student engagement and retention. These results
suggest that student evaluations of teaching can be reliable indicators
of teaching effectiveness, but likely only within the same courses and
semester. Thus, a multifaceted approach to assesing instructor
effectiveness and improvement is a more appropriate measurement tool for
long-term evaluations of instruction (Benton \& Young, 2018).

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-arubayi1987}{}}%
Arubayi, E. A. (1987). Improvement of instruction and teacher effectiveness: are student ratings reliable and valid? \emph{Higher Education}, \emph{16}(3), 267--278. \url{https://doi.org/10.1007/BF00148970}

\leavevmode\vadjust pre{\hypertarget{ref-aust2022}{}}%
Aust, F., Barth, M., Diedenhofen, B., Stahl, C., Casillas, J. V., \& Siegel, R. (2022). \emph{Papaja: Prepare american psychological association journal articles with r markdown}. \url{https://CRAN.R-project.org/package=papaja}

\leavevmode\vadjust pre{\hypertarget{ref-barton2020}{}}%
Bartoń, K. (2020). \emph{MuMIn: Multi-model inference}. \url{https://CRAN.R-project.org/package=MuMIn}

\leavevmode\vadjust pre{\hypertarget{ref-becker2021}{}}%
Becker, J., Chan, C., Chan, G. C., Leeper, T. J., Gandrud, C., MacDonald, A., Zahn, I., Stadlmann, S., Williamson, R., Kennedy, P., Price, R., Davis, T. L., Day, N., Denney, B., \& Bokov, A. (2021). \emph{Rio: A swiss-army knife for data i/o}. \url{https://cran.r-project.org/web/packages/rio/}

\leavevmode\vadjust pre{\hypertarget{ref-benton2014}{}}%
Benton, S. L., \& Cashin, W. E. (2014). \emph{Student Ratings of Instruction in College and University Courses} (M. B. Paulsen, Ed.; pp. 279--326). Springer Netherlands. \url{https://doi.org/10.1007/978-94-017-8005-6_7}

\leavevmode\vadjust pre{\hypertarget{ref-benton2016}{}}%
Benton, S. L., \& Ryalls, K. R. (2016). \emph{Challenging Misconceptions about Student Ratings of Instruction. IDEA Paper {\#}58}. \url{https://eric.ed.gov/?id=ED573670}

\leavevmode\vadjust pre{\hypertarget{ref-benton2018}{}}%
Benton, S. L., \& Young, S. (2018). \emph{Best Practices in the Evaluation of Teaching. IDEA Paper {\#}69}. \url{https://eric.ed.gov/?id=ED588352}

\leavevmode\vadjust pre{\hypertarget{ref-berk2018}{}}%
Berk, R. A. (2018). Start Spreading the News: Use Multiple Sources of Evidence to Evaluate Teaching. \emph{The Journal of Faculty Development}, \emph{31}(1), 73--81. \url{https://www.schreyerinstitute.psu.edu/pdf/UseMultipleSourcesSRs_Berk_JFacDev1-11-2018.pdf}

\leavevmode\vadjust pre{\hypertarget{ref-boring2016}{}}%
Boring, A., Ottoboni, K., \& Stark, P. B. (2016). Student evaluations of teaching (mostly) do not measure teaching effectiveness. \emph{ScienceOpen Research}, \emph{0}(0). \url{https://doi.org/10.14293/S2199-1006.1.SOR-EDU.AETBZC.v1}

\leavevmode\vadjust pre{\hypertarget{ref-boswell2016}{}}%
Boswell, S. S. (2016). Ratemyprofessors is hogwash (but I care): Effects of Ratemyprofessors and university-administered teaching evaluations on professors. \emph{Computers in Human Behavior}, \emph{56}, 155--162. \url{https://doi.org/10.1016/j.chb.2015.11.045}

\leavevmode\vadjust pre{\hypertarget{ref-chen2017}{}}%
Chen, C. Y., Wang, S.-Y., \& Yang, Y.-F. (2017). A Study of the Correlation of the Improvement of Teaching Evaluation Scores Based on Student Performance Grades. \emph{International Journal of Higher Education}, \emph{6}(2), 162. \url{https://doi.org/10.5430/ijhe.v6n2p162}

\leavevmode\vadjust pre{\hypertarget{ref-cohen2003}{}}%
Cohen, J., Cohen, P., West, S. G., \& Aiken, L. (2003). \emph{Applied multiple regression / correlation analysis for the behavioral sciences} (3rd ed.). Lawrence Erlbaum Associates.

\leavevmode\vadjust pre{\hypertarget{ref-cohen1981}{}}%
Cohen, P. A. (1981). Student Ratings of Instruction and Student Achievement: A Meta-analysis of Multisection Validity Studies. \emph{Review of Educational Research}, \emph{51}(3), 281--309. \url{https://doi.org/10.3102/00346543051003281}

\leavevmode\vadjust pre{\hypertarget{ref-crumbley2001}{}}%
Crumbley, L., Henry, B. K., \& Kratchman, S. H. (2001). Students{'} perceptions of the evaluation of college teaching. \emph{Quality Assurance in Education}, \emph{9}(4), 197--207. \url{https://doi.org/10.1108/EUM0000000006158}

\leavevmode\vadjust pre{\hypertarget{ref-ben-shachar2023}{}}%
\emph{Effectsize: Indices of effect size}. (2023). {[}Computer software{]}. \url{https://cran.r-project.org/web/packages/effectsize/}

\leavevmode\vadjust pre{\hypertarget{ref-esarey2020}{}}%
Esarey, J., \& Valdes, N. (2020). Unbiased, reliable, and valid student evaluations can still be unfair. \emph{Assessment \& Evaluation in Higher Education}, \emph{45}(8), 1106--1120. \url{https://doi.org/10.1080/02602938.2020.1724875}

\leavevmode\vadjust pre{\hypertarget{ref-fan2019}{}}%
Fan, Y., Shepherd, L. J., Slavich, E., Waters, D., Stone, M., Abel, R., \& Johnston, E. L. (2019). Gender and cultural bias in student evaluations: Why representation matters. \emph{PLOS ONE}, \emph{14}(2), e0209749. \url{https://doi.org/10.1371/journal.pone.0209749}

\leavevmode\vadjust pre{\hypertarget{ref-feistauer2017}{}}%
Feistauer, D., \& Richter, T. (2017). How reliable are students{'} evaluations of teaching quality? A variance components approach. \emph{Assessment \& Evaluation in Higher Education}, \emph{42}(8), 1263--1279. \url{https://doi.org/10.1080/02602938.2016.1261083}

\leavevmode\vadjust pre{\hypertarget{ref-felton2008}{}}%
Felton, J., Koper, P. T., Mitchell, J., \& Stinson, M. (2008). Attractiveness, easiness and other issues: student evaluations of professors on Ratemyprofessors.com. \emph{Assessment \& Evaluation in Higher Education}, \emph{33}(1), 45--61. \url{https://doi.org/10.1080/02602930601122803}

\leavevmode\vadjust pre{\hypertarget{ref-freishtat2014}{}}%
Freishtat, R. (2014). An evaluation of course evaluations. \emph{ScienceOpen Research}. \url{https://doi.org/10.14293/S2199-1006.1.SOR-EDU.AOFRQA.v1}

\leavevmode\vadjust pre{\hypertarget{ref-gelman2006}{}}%
Gelman, A. (2006). Multilevel (hierarchical) modeling: What it can and cannot do. \emph{Technometrics}, \emph{48}(3), 432--435. \url{https://doi.org/10.1198/004017005000000661}

\leavevmode\vadjust pre{\hypertarget{ref-gillmore1978}{}}%
Gillmore, G. M., Kane, M. T., \& Naccarato, R. W. (1978). The generalizability of student ratings of instruction: Estimation of the teacher and course components. \emph{Journal of Educational Measurement}, \emph{15}(1), 1--13. \url{https://www.jstor.org/stable/1433721}

\leavevmode\vadjust pre{\hypertarget{ref-greenwald1997}{}}%
Greenwald, A. G., \& Gillmore, G. M. (1997). Grading leniency is a removable contaminant of student ratings. \emph{American Psychologist}, \emph{52}(11), 1209--1217. \url{https://doi.org/10.1037/0003-066X.52.11.1209}

\leavevmode\vadjust pre{\hypertarget{ref-hattie1996}{}}%
Hattie, J., \& Marsh, H. W. (1996). The Relationship Between Research and Teaching: A Meta-Analysis. \emph{Review of Educational Research}, \emph{66}(4), 507--542. \url{https://doi.org/10.3102/00346543066004507}

\leavevmode\vadjust pre{\hypertarget{ref-heffernan2022}{}}%
Heffernan, T. (2022). Sexism, racism, prejudice, and bias: A literature review and synthesis of research surrounding student evaluations of courses and teaching. \emph{Assessment \& Evaluation in Higher Education}, \emph{47}(1), 144--154. \url{https://doi.org/10.1080/02602938.2021.1888075}

\leavevmode\vadjust pre{\hypertarget{ref-horan2010}{}}%
Horan, S. M., Chory, R. M., \& Goodboy, A. K. (2010). Understanding students' classroom justice experiences and responses. \emph{Communication Education}, \emph{59}(4), 453--474. \url{https://doi.org/10.1080/03634523.2010.487282}

\leavevmode\vadjust pre{\hypertarget{ref-hornstein2017}{}}%
Hornstein, H. A. (2017). Student evaluations of teaching are an inadequate assessment tool for evaluating faculty performance. \emph{Cogent Education}, \emph{4}(1), 1304016. \url{https://doi.org/10.1080/2331186X.2017.1304016}

\leavevmode\vadjust pre{\hypertarget{ref-johnson2013}{}}%
Johnson, M. D., Narayanan, A., \& Sawaya, W. J. (2013). Effects of Course and Instructor Characteristics on Student Evaluation of Teaching across a College of Engineering: Student Evaluation of Teaching across a College of Engineering. \emph{Journal of Engineering Education}, \emph{102}(2), 289--318. \url{https://doi.org/10.1002/jee.20013}

\leavevmode\vadjust pre{\hypertarget{ref-kim2015}{}}%
Kim, S. (2015). \emph{Ppcor: Partial and semi-partial (part) correlation}. \url{https://cran.r-project.org/web/packages/ppcor/}

\leavevmode\vadjust pre{\hypertarget{ref-kornell2016}{}}%
Kornell, N., \& Hausman, H. (2016). Do the best teachers get the best ratings? \emph{Frontiers in Psychology}, \emph{7}. \url{https://doi.org/10.3389/fpsyg.2016.00570}

\leavevmode\vadjust pre{\hypertarget{ref-leventhal1980}{}}%
Leventhal, G. S. (1980). \emph{What Should Be Done with Equity Theory?} (K. J. Gergen, M. S. Greenberg, \& R. H. Willis, Eds.; pp. 27--55). Springer US. \url{https://doi.org/10.1007/978-1-4613-3087-5_2}

\leavevmode\vadjust pre{\hypertarget{ref-luxfcdecke2023}{}}%
Lüdecke, D., Makowski, D., Ben-Shachar, M. S., Patil, I., Højsgaard, S., Wiernik, B. M., Lau, Z. J., Arel-Bundock, V., Girard, J., Maimone, C., Ohlsen, N., Morrison, D. E., \& Luchman, J. (2023). \emph{Parameters: Processing of model parameters}. \url{https://CRAN.R-project.org/package=parameters}

\leavevmode\vadjust pre{\hypertarget{ref-macnell2015}{}}%
MacNell, L., Driscoll, A., \& Hunt, A. N. (2015). What{'}s in a Name: Exposing Gender Bias in Student Ratings of Teaching. \emph{Innovative Higher Education}, \emph{40}(4), 291--303. \url{https://doi.org/10.1007/s10755-014-9313-4}

\leavevmode\vadjust pre{\hypertarget{ref-marks2000}{}}%
Marks, R. B. (2000). Determinants of Student Evaluations of Global Measures of Instructor and Course Value. \emph{Journal of Marketing Education}, \emph{22}(2), 108--119. \url{https://doi.org/10.1177/0273475300222005}

\leavevmode\vadjust pre{\hypertarget{ref-marsh2007}{}}%
Marsh, H. W. (2007). Do university teachers become more effective with experience? A multilevel growth model of students' evaluations of teaching over 13 years. \emph{Journal of Educational Psychology}, \emph{99}(4), 775--790. \url{https://doi.org/10.1037/0022-0663.99.4.775}

\leavevmode\vadjust pre{\hypertarget{ref-marsh1997}{}}%
Marsh, H. W., \& Roche, L. A. (1997). Making students' evaluations of teaching effectiveness effective: The critical issues of validity, bias, and utility. \emph{American Psychologist}, \emph{52}(11), 1187--1197. \url{https://doi.org/10.1037/0003-066X.52.11.1187}

\leavevmode\vadjust pre{\hypertarget{ref-mitchell2018}{}}%
Mitchell, K. M. W., \& Martin, J. (2018). Gender Bias in Student Evaluations. \emph{PS: Political Science \& Politics}, \emph{51}(3), 648--652. \url{https://doi.org/10.1017/S104909651800001X}

\leavevmode\vadjust pre{\hypertarget{ref-overall1980}{}}%
Overall, J. U., \& Marsh, H. W. (1980). Students' evaluations of instruction: A longitudinal study of their stability. \emph{Journal of Educational Psychology}, \emph{72}(3), 321--325. \url{https://doi.org/10.1037/0022-0663.72.3.321}

\leavevmode\vadjust pre{\hypertarget{ref-pepper2008}{}}%
Pepper, M. B., \& Pathak, S. (2008). Classroom contribution: What do students perceive as fair assessment? \emph{Journal of Education for Business}, \emph{83}(6), 360--368. \url{https://doi.org/10.3200/JOEB.83.6.360-368}

\leavevmode\vadjust pre{\hypertarget{ref-pinheiro2017}{}}%
Pinheiro, J., Bates, D., Debroy, S., Sarkar, D., \& Team, R. C. (2017). \emph{Nlme: Linear and nonlinear mixed effects models}. \url{https://cran.r-project.org/package=nlme}

\leavevmode\vadjust pre{\hypertarget{ref-rantanen2012}{}}%
Rantanen, P. (2012). The number of feedbacks needed for reliable evaluation. A multilevel analysis of the reliability, stability and generalisability of students{'} evaluation of teaching. \emph{Assessment \& Evaluation in Higher Education}, \emph{38}(2), 224--239. \url{https://doi.org/10.1080/02602938.2011.625471}

\leavevmode\vadjust pre{\hypertarget{ref-rovai2006}{}}%
Rovai, A. P., Ponton, M. K., Derrick, M. G., \& Davis, J. M. (2006). Student evaluation of teaching in the virtual and traditional classrooms: A comparative analysis. \emph{The Internet and Higher Education}, \emph{9}(1), 23--35. \url{https://doi.org/10.1016/j.iheduc.2005.11.002}

\leavevmode\vadjust pre{\hypertarget{ref-sheehan1975}{}}%
Sheehan, D. S. (1975). On the Invalidity of Student Ratings for Administrative Personnel Decisions. \emph{The Journal of Higher Education}, \emph{46}(6), 687--700. \url{https://doi.org/10.1080/00221546.1975.11778669}

\leavevmode\vadjust pre{\hypertarget{ref-smith2011}{}}%
Smith, B. P., \& Hawkins, B. (2011). Examining student evaluations of black college faculty: Does race matter? \emph{The Journal of Negro Education}, \emph{80}(2), 149--162. \url{https://www.jstor.org/stable/41341117}

\leavevmode\vadjust pre{\hypertarget{ref-spooren2013}{}}%
Spooren, P., Brockx, B., \& Mortelmans, D. (2013). On the Validity of Student Evaluation of Teaching: The State of the Art. \emph{Review of Educational Research}, \emph{83}(4), 598--642. \url{https://doi.org/10.3102/0034654313496870}

\leavevmode\vadjust pre{\hypertarget{ref-tabachnick2019}{}}%
Tabachnick, B. G., Fidell, L. S., \& Ullman, J. B. (2019). \emph{Using multivariate statistics} (Seventh edition). Pearson.

\leavevmode\vadjust pre{\hypertarget{ref-tata1999}{}}%
Tata, J. (1999). Grade distributions, grading procedures, and students' evaluations of instructors: A justice perspective. \emph{The Journal of Psychology}, \emph{133}(3), 263--271. \url{https://doi.org/10.1080/00223989909599739}

\leavevmode\vadjust pre{\hypertarget{ref-tripp2019}{}}%
Tripp, T. M., Jiang, L., Olson, K., \& Graso, M. (2019). The Fair Process Effect in the Classroom: Reducing the Influence of Grades on Student Evaluations of Teachers. \emph{Journal of Marketing Education}, \emph{41}(3), 173--184. \url{https://doi.org/10.1177/0273475318772618}

\leavevmode\vadjust pre{\hypertarget{ref-uttl2017}{}}%
Uttl, B., White, C. A., \& Gonzalez, D. W. (2017). Meta-analysis of faculty's teaching effectiveness: Student evaluation of teaching ratings and student learning are not related. \emph{Studies in Educational Evaluation}, \emph{54}, 22--42. \url{https://doi.org/10.1016/j.stueduc.2016.08.007}

\leavevmode\vadjust pre{\hypertarget{ref-weaver2014}{}}%
Weaver, B., \& Koopman, R. (2014). An SPSS macro to compute confidence intervals for pearson{'}s correlation. \emph{The Quantitative Methods for Psychology}, \emph{10}(1), 29--39. \url{https://doi.org/10.20982/tqmp.10.1.p029}

\leavevmode\vadjust pre{\hypertarget{ref-R-ggplot2}{}}%
Wickham, H. (2016). \emph{ggplot2: Elegant graphics for data analysis}. Springer-Verlag New York. \url{https://ggplot2.tidyverse.org}

\leavevmode\vadjust pre{\hypertarget{ref-R-dplyr}{}}%
Wickham, H., François, R., Henry, L., \& Kirill Müller. (2020). \emph{Dplyr: A grammar of data manipulation}. \url{https://CRAN.R-project.org/package=dplyr}

\leavevmode\vadjust pre{\hypertarget{ref-zhao2011}{}}%
Zhao, J., \& Gallant, D. J. (2011). Student evaluation of instruction in higher education: exploring issues of validity and reliability. \emph{Assessment \& Evaluation in Higher Education}. \url{https://www.tandfonline.com/doi/full/10.1080/02602938.2010.523819}

\leavevmode\vadjust pre{\hypertarget{ref-zheng2023}{}}%
Zheng, X., Vastrad, S., He, J., \& Ni, C. (2023). Contextualizing gender disparities in online teaching evaluations for professors. \emph{PLOS ONE}, \emph{18}(3), e0282704. \url{https://doi.org/10.1371/journal.pone.0282704}

\end{CSLReferences}


\end{document}
