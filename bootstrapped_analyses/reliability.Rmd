---
title             : "Reliability of Instructor Evaluations"
shorttitle        : "RELIABILITY EVALUATIONS"
author: 
  - name          : "Erin M. Buchanan"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "326 Market St., Harrisburg, PA 17010"
    email         : "ebuchanan@harrisburgu.edu"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
      - "Analysis"
  - name          : "Jacob Miranda"
    affiliation   : "2"
    role:
      - "Writing - Original Draft Preparation"
  - name          : "Christian Stephens"
    affiliation   : "2"
    role:
      - "Writing - Original Draft Preparation"
affiliation:
  - id            : "1"
    institution   : "Harrisburg University of Science and Technology"
  - id            : "2"
    institution   : "University of Alabama"
authornote: |
  TBA
abstract: |
  TBA
  
keywords          : "keywords"
wordcount         : "X"
bibliography      : "includes/grade_references.bib"
floatsintext      : no
linenumbers       : yes
draft             : no
mask              : no
figurelist        : no
tablelist         : no
footnotelist      : no
classoption       : "man"
output            : papaja::apa6_docx
editor_options: 
  chunk_output_type: inline
---

```{r setup, include = FALSE}
library("papaja")
r_refs("includes/r-references.bib", append = TRUE)
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(89432289)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, echo = FALSE, include = FALSE)
library(rio)
library(dplyr)
library(nlme)
library(effectsize)
library(ggplot2)
library(ppcor)
library(MuMIn)
```

The following was pre-registered: https://osf.io/czb4f

Exploratory Research Questions:

1)  What is the reliability of instructor evaluations?
2)  Are instructor evaluations reliable across time?
3)  Is the average level of perceived fairness of the grading in the course a moderator of reliability in instructor evaluations?
4)  Does the average variability in instructor fairness rating moderate
    reliability of instructor evaluations?

# Method

## Data Source

```{r import-data}
# please note the data is deidentified but timing and course number is accurate
DF <- import("data/final_evals.csv")

# we will only use courses with at least 15 ratings ... look at the paper that may be reliable (2013)
DF_sample <- DF %>% 
  filter(People >= 15)

# for this pre-reg we will randomly sample 20 percent of instructors 
# random_sample <- sample(unique(DF_sample$instructor_code), 
#                         size = round(length(unique(DF_sample$instructor_code)) * .30),
#                         replace = FALSE)
# 
# DF_sample <- DF_sample %>% 
#   filter(instructor_code %in% random_sample)
```

The archival study was conducted using data from the psychology
department at a large Midwestern public university. We used data from
`r nrow(DF_sample[ DF_sample$course_level == "undergraduate" , ])`
undergraduate, `r nrow(DF_sample[ DF_sample$course_level == "mixed", ])`
mixed-level undergraduate, and
`r nrow(DF_sample[ DF_sample$course_level == "masters" , ])` graduate
psychology classes taught from 1987 to 2018 that were evaluated by
students using the same 15-item instrument. Faculty followed set
procedures in distributing scan forms no more than two weeks before the
conclusion of the semester. A student was assigned to collect the forms
and deliver them to the departmental secretary. The instructor was
required to leave the room while students completed the forms. In the
last several years of evaluations, online versions of these forms were
used with faculty encouraged to give students time to complete them in
class while they were outside the classroom.

The questionnaire given to students can be found at
<https://osf.io/4sphx>. These items were presented with a five-point
scale from 1 (*strongly disagree*) to 5 (*strongly agree*). For this
study, the overall instructor evaluation question was "The overall
quality of this course was among the top 20% of those I have taken.".
For fairness, we used the question of "The instructor used fair and
appropriate methods in the determination of grades.". The ratings were
averaged for each course, and the sample size for each rating was
included.

## Planned Analyses

The evaluations were filtered for those with at least fifteen student
ratings for the course [@rantanen2012]. We performed a robustness
check for the first research question on the data when the sample size
is at least *n* = 10 up to *n* = 14 (i.e., on all evaluations with at
least 10 ratings, then at least 11 ratings, etc.) to determine if the
reliability estimates are stable at lower sample sizes. We first
screened the dataset (two evaluation questions, sample size for course)
for accuracy errors, linearity, normality, and homoscedasticity. The
data is assumed to not have traditional "outliers", as these evaluations
represent true averages from student evaluations. If the linearity
assumption fails, we considered potential nonparametric models to
address non-linearity. Deviations from normality were noted as the
large sample size should provide robustness for any violations of
normality. If data appears to be heteroscedastic, we used
bootstrapping to provide estimates and confidence intervals.

This data was considered structured by instructor; therefore, all
analyses below were coded in *R* using the *nlme* package
[@pinheiro2017] to control for correlated error of instructor as a
random intercept in a multilevel model. Multilevel models allow for
analysis of repeated measures data without collapsing by participant
[i.e., each instructor/semester/course combination can be kept separate
without averaging over these measurements; @gelman2006]. Random
intercept models are regression models on repeated data that structure
the data by a specified variable, which was instructor in this analysis.
Therefore, each instructor's average rating score was allowed to vary
within the analysis, as ratings would be expected to be different from
instructor to instructor. In each of the analyses described below, the
number of students providing ratings for the course was included as a
control variable to even out differences in course size as an influence
in the results. However, this variable was excluded if the models did
not converge. The dependent variable and predictors varied based on the
research question, and these are described with each analysis below.

### RQ 1 

In this research question, we examined the reliability of instructor
evaluations on the overall rating and separately on the fairness rating.
We calculated eight types of reliability using course (same or
different) by instructor (same or different) by semester (same or
different). The dependent variable was the first question average
with a predictor of the comparison question average, and both sample
sizes (first sample size, comparison sample size). Instructor code was used 
as the random intercept for both ratings (i.e., two instructor
random intercepts, first and comparison). The value of interest was the
standardized regression coefficient for the fixed effect of question
from this model. Given that the large sample size will likely produce
"significant" *p*-values, we used the 95% CI to determine which
reliability values were larger than zero and to compare reliability
estimates to each other.

### RQ 2

We used the reliability for the same instructor and course
calculated as described in RQ1 at each time point difference between
semesters. For example, the same semester would create a time difference
of 0. The next semester (Spring to Summer, Summer to Fall, Fall to
Spring) would create a time difference of 1. We used the time
difference as a fixed effect to predict reliability for the overall
question only with a random intercept of instructor. We used the
coefficient of time difference and its confidence interval to determine
if there was a linear change over time. Finally, we plotted the changes
over time to examine if this effect was non-linear in nature and discuss
implications of the graph.

### RQ 3

Using the reliability estimates from RQ 2, we then added the average
rating for the fairness question as the moderator with time to predict
reliability. Fairness was calculated as the average of the fairness
question for all courses involved in the reliability calculation for
that instructor and time difference. Therefore, this rating represented
the average perceived fairness of grading at the time of ratings. If
this interaction effect's coefficient does not include zero, we performed a 
simple slopes analysis to examine the effects of instructors
who were rated at average fairness, one standard deviation below average,
and one standard deviation above average [@cohen2003]. 

### RQ 4

Finally, we examined the average standard deviation of fairness ratings as a 
moderator of with time to predict reliability. This variable
represented the variability in perceived fairness in grading from student
evaluations, where small numbers indicated relative agreement on the
rating of fairness and larger values indicated a wide range of fairness
ratings. The variability in fairness ratings was calculated in the same way 
as the mean fairness, which was only for the instructor and semester time 
difference evaluations that were used to calculate the reliability estimate. 
This research question was assessed the same way as research question three. 

# Results

## Data Screening

```{r datascreening, include = FALSE}
random <- rchisq(nrow(DF_sample), 7)
output <- lm(random ~ ., data = DF_sample %>% dplyr::select(People, Q1AVG, Q4AVG))
standardized <- rstudent(output)
fitvalues <- scale(output$fitted.values)

#linear
{qqnorm(standardized)
abline(0,1)}

#multivariate normality
hist(standardized, breaks=15)

#homogeneity and homoscedasticity
{plot(fitvalues, standardized) 
abline(0,0)
abline(v = 0)}
```

The overall dataset was screened for normality, linearity, homogeneity,
and homoscedasticity using procedures from @tabachnick2019. Data
generally met assumptions with a slight skew and some heterogeneity. 
The complete anonymized dataset and other information can be found
online at <https://osf.io/k7zh2>. This page also includes the manuscript
written inline with the statistical analysis with the *papaja* package
[@aust2022] for interested researchers/reviewers who wish to recreate
these analyses. The bootstrapped versions of analyses and sensitivity analysis 
can be found online on our OSF page with a summary of results. 

## Descriptive Statistics

`r nrow(DF_sample)` evaluations included at least 15 student evaluations
for analysis. Table \@ref(tab:table1) portrays the descriptive
statistics for each course level including the total number of
evaluations, unique instructors, unique course numbers, and average
scores for the two rating items. Students additionally projected their
course grade for each class (*A* = 5, *B* = 4, *C* = 3, *D* = 2, *F* =
1), and the average for this item is included for reference. Overall,
`r length(unique(DF_sample$instructor_code))` unique instructors and
`r length(unique(DF_sample$new_course))` unique courses were included in
the analyses below across `r max(DF_sample$semester_count) + 1`
semesters.

```{r table1, results = 'asis', include = TRUE}
DF_summary <- DF_sample %>% 
  group_by(course_level) %>% 
  summarize(totaln = n(), 
            num_instruct = length(unique(instructor_code)),
            num_courses = length(unique(new_course)),
            avg_people = apa_num(mean(People, na.rm = T)), 
            avgq1 = apa_num(mean(Q1AVG, na.rm = T)), 
            avgsd1 = apa_num(sd(Q1AVG, na.rm = T)), 
            avgq4 = apa_num(mean(Q4AVG, na.rm = T)),
            avgsd4 = apa_num(sd(Q4AVG, na.rm = T)),
            avgq15 = apa_num(mean(Q15AVG, na.rm = T)), 
            avgsd15 = apa_num(sd(Q15AVG, na.rm = T))) %>% 
  arrange(desc(course_level))

temp <- DF_summary %>% 
  t() %>% 
  as.data.frame() %>% 
  slice(-1)

temp$stat <- c("N Total", "N Instructors", "N Courses",
               "Average N Ratings", "Average Overall", 
               "SD Overall", "Average Fairness", 
               "SD Fairness", "Average Grade", "SD Grade")
apa_table(temp[ , c(4, 1:3)], 
          caption = "Descriptive Statistics of Included Courses", 
            row.names = FALSE, 
            col.names = c("Statistic", "Undergraduate", "Mixed", "Master's"))
```

## RQ 1

```{r rq1-nlme}
# let's see if twitter can help here - yes! @smartin2018 for the win!

# create unique id
DF_sample$evalid <- 1:nrow(DF_sample)

# create every pairwise combination 
combns <- t(combn(DF_sample$evalid,2))

# create a dataframe to hold that information, remove other frame for the love of space
longrel <- data.frame(evalid=combns[,1],evalid2=combns[,2]); rm(combns)

# add person 1 information back to dataframe
templong <- merge(longrel, DF_sample, by = "evalid")
colnames(templong)[1:2] <- c("evalid1", "evalid")

# add person 2 information back to the dataframe, remove other frame for the love of space
longrel <- merge(templong, DF_sample, by = "evalid")
rm(templong)

# create factors
longrel$instructormatch <- as.numeric(longrel$instructor_code.x == longrel$instructor_code.y)
longrel$semestermatch <- as.numeric(longrel$semester_count.x == longrel$semester_count.y)
longrel$coursematch <- as.numeric(longrel$new_course.x == longrel$new_course.y)

#look at the contingency table
#table(longrel$instructormatch, longrel$semestermatch, longrel$coursematch)

# dataframe to put stuff in ----
betarel <- matrix(NA, nrow = 2*2*2*2, ncol = 4+3)

# going to let these overwrite each other so memory isn't bonkers
# semester, course, instructor
## same same same ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y),
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[1 , ] <- c(1,1,1,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[2 , ] <- c(1,1,1,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

## same same different ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[3 , ] <- c(1,1,0,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[4 , ] <- c(1,1,0,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

## same different same ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[5 , ] <- c(1,0,1,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[6 , ] <- c(1,0,1,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

## same different different ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[7 , ] <- c(1,0,0,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[8 , ] <- c(1,0,0,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

## different different different ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[9 , ] <- c(0,0,0,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[10 , ] <- c(0,0,0,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

## different same different ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[11 , ] <- c(0,1,0,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[12 , ] <- c(0,1,0,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

## different different same ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[13 , ] <- c(0,0,1,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[14 , ] <- c(0,0,1,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

## different same same ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[15 , ] <- c(0,1,1,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[16 , ] <- c(0,1,1,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

betarel <- as.data.frame(betarel)
colnames(betarel) <- c("Semester", "Course", "Instructor", "Question",
                      "b", "CI_Low", "CI_High")
```

Each individual evaluation was compared to every other evaluation
resulting in `r nrow(longrel)` total comparisons. Eight combinations of
ratings were examined using instructor (same, different), course (same,
different), and semester (same, different) on both the overall and
fairness evaluation ratings separately. One of the individual ratings was used to predict the comparison rating (i.e., question 1 was used to predict a comparison question 1 for the same instructor, different instructor, same semester, different semester, etc.), and the number of ratings (i.e., rating sample size) per question were used as fixed-effects covariates. The instructor(s) were used as a random intercept to
control for correlated error and overall average rating per instructor.
The effects were then standardized using the *parameters* package
[@lÃ¼decke2023]. The data was sorted by year and semester such that "predictor" was always an earlier semester predicting a later semester's scores, except in cases of the the same semester comparisons. Therefore, positive standardized scores indicate that scores tend to go up over time, while negative scores indicate that scores tend to go down over time. 

As shown in \@ref(fig:figure1), reliability was highest when calculated
on the same instructor in the same semester and within the same course for both overall rating and fairness. This reliability was followed by the same instructor, same semester, and different courses. Next, the reliability for same instructor, same
course, and different semesters was greater than zero and usually
overlapped in confidence interval with same instructor, same semester,
and different courses. Interestingly, the same instructor with different courses and semesters showed a non-zero negative relationship, indicating that ratings generally were lower for later semesters in different courses. 

For different instructors, we found positive non-zero reliablities when they were at least calculated on the same semester or course. These values were very close to zero, generally in the .01 to .05 range. Last, the reliabilities that were calculated on different courses, semesters, and instructors include zero in their confidence intervals. Exact values can be found in the online supplemental document with the robustness analysis. 

ROBUSTNESS REVEALED: 

```{r figure1, include = TRUE, fig.cap= "Reliability estimates for instructor, course, and semester combinations."}
betarel$b <- as.numeric(betarel$b)
betarel$CI_Low <- as.numeric(betarel$CI_Low)
betarel$CI_High <- as.numeric(betarel$CI_High)
betarel$Semester_nice <- factor(betarel$Semester, 
                           levels = c(0,1),
                           labels = c("Different", "Same"))
betarel$Instructor_nice <- factor(betarel$Instructor, 
                           levels = c(0,1),
                           labels = c("Different Instructor", "Same Instructor"))
betarel$Course_nice <- factor(betarel$Course, 
                           levels = c(0,1),
                           labels = c("Different Course", "Same Course"))

write.csv(betarel, "output/beta_values_rq1.csv", row.names = F)

ggplot(betarel, aes(Semester_nice, b, color = Course_nice)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = CI_Low, ymax = CI_High), width = 0.2) + 
  theme_classic() + 
  facet_grid(Question~Instructor_nice) + 
  ylab("Reliability") + 
  xlab("Semester Match") + 
  scale_color_discrete(name = "Course Match")

ggsave("output/figure1.png")
```

## RQ 2

```{r rq2-setup, warning = FALSE}
# filter to only matches
long_match <- longrel %>% 
  filter(coursematch == 1) %>% 
  filter(instructormatch == 1) %>% 
  dplyr::select(instructor_code.x, semester_count.x, People.x, Q1AVG.x, Q4AVG.x, 
         instructor_code.y, semester_count.y, People.y, Q1AVG.y, Q4AVG.y) %>% 
  mutate(semester_diff = round(semester_count.y - semester_count.x))

# figure out the number each person has and need at least 10
long_summary <- long_match %>% 
  group_by(instructor_code.x, semester_diff) %>% 
  summarize(total_n = n(), .groups = "keep", 
            avg_fair = mean(c(Q4AVG.x, Q4AVG.y), na.rm = T), 
            std_fair = sd(c(Q4AVG.x, Q4AVG.y), na.rm = T)) %>% 
  filter(total_n >= 10) %>% 
  mutate(code = paste0(instructor_code.x, semester_diff))

long_match$code <- paste0(long_match$instructor_code.x, long_match$semester_diff)

save_rel <- data.frame(
  code = 1:nrow(long_summary), 
  rel = 1:nrow(long_summary)
)

for (i in 1:nrow(long_summary)){
  temp <- long_match %>% 
    filter(code %in% long_summary$code[i]) %>% 
    dplyr::select(Q1AVG.x, Q1AVG.y, People.x, People.y) %>% 
    pcor()
  save_rel$code[i] <- long_summary$code[i]
  save_rel$rel[i] <- temp$estimate[2, 1]
}

save_rel <- save_rel %>% 
  left_join(long_summary %>% dplyr::select(code, instructor_code.x, semester_diff, avg_fair, std_fair) %>% unique(), by = "code")
```

The paired evaluations were then filtered to only examine course and
instructor matches to explore the relation of reliability across time.
Reliability was calculated by calculating the partial correlation
between the overall rating for the course first evaluation and the
overall rating for the course second evaluation, controlling for the
number of ratings within those average scores. This reliability was
calculated separately for each instructor and semester difference (i.e.,
the time between evaluations, 0 means same semester, 1 means the next
semester, 2 means two semesters later, etc.). The ratings were filtered
so that at least 10 pairs of ratings were present for each instructor
and semester difference combination [@weaver2014]. Of
`r nrow(long_match)` possible matched instructor and course pairings,
`r nrow(long_match %>% filter(code %in% long_summary$code))` included at
least 10 pairings, which was `r nrow(long_summary)` total instructor and
semester combinations.

```{r rq2-nlme}
rq2.model <- lme(rel ~ semester_diff, 
                 data = save_rel,
                 random = list(~1|instructor_code.x), 
                 method = "ML", 
                 na.action = "na.omit",
                 control = lmeControl(opt = "optim"))

coef.rq2 <- summary(rq2.model)$tTable
ci.rq2 <- intervals(rq2.model)
```

The confidence interval for the effect of semester difference predicting
reliability did not cross zero, *b* = `r apa_num(coef.rq2[2 , 1], digits = 3)`,
95% CI [`r apa_num(ci.rq2$fixed[2,1], digits = 3)`, `r apa_num(ci.rq2$fixed[2,3], digits = 3)`],
$R^2$ =
`r suppressWarnings(apa_num(as.numeric(r.squaredGLMM(rq2.model)[1]), gt1 = FALSE))`. The coefficient, while small, represents a small effect of time on the reliability of instructor ratings. As shown in \@ref(fig:figure2), reliability appears to decrease across time. 

```{r figure2, include = TRUE, fig.cap = "Reliability estimates for same instructor and course across time.", message = F}
ggplot(save_rel, aes(semester_diff, rel)) + 
  geom_point(alpha = .5) + 
  theme_classic() + 
  xlab("Semester Difference") + 
  ylab("Reliability") + 
  geom_smooth(method = "lm", color = "black") 

ggsave("output/figure2.png")
```

## RQ 3

```{r rq3-nlme}
# center the variables
save_rel$avg_fair_center <- scale(save_rel$avg_fair, scale = FALSE)
save_rel$avg_fair_low <- save_rel$avg_fair_center + sd(save_rel$avg_fair_center, na.rm = T)
save_rel$avg_fair_high <- save_rel$avg_fair_center - sd(save_rel$avg_fair_center, na.rm = T)
save_rel$z_semester_diff <- scale(save_rel$semester_diff, scale = FALSE)

rq3.model <- lme(rel ~ z_semester_diff*avg_fair_center, 
                 data = save_rel,
                 random = list(~1|instructor_code.x), 
                 method = "ML", 
                 na.action = "na.omit",
                 control = lmeControl(opt = "optim"))

coef.rq3 <- summary(rq3.model)$tTable
ci.rq3 <- intervals(rq3.model)

rq3.model.low <- lme(rel ~ z_semester_diff*avg_fair_low, 
                 data = save_rel,
                 random = list(~1|instructor_code.x), 
                 method = "ML", 
                 na.action = "na.omit",
                 control = lmeControl(opt = "optim"))
coef.rq3.low <- summary(rq3.model.low)$tTable

rq3.model.high <- lme(rel ~ z_semester_diff*avg_fair_high, 
                 data = save_rel,
                 random = list(~1|instructor_code.x), 
                 method = "ML", 
                 na.action = "na.omit",
                 control = lmeControl(opt = "optim"))
coef.rq3.high <- summary(rq3.model.high)$tTable
```

The confidence interval for the interaction of semester time difference
and average fairness did cross zero, *b* =
`r apa_num(coef.rq3[4,1], digits = 3)`, 95% CI [`r apa_num(ci.rq3$fixed[4, 1], digits = 3)`,
`r apa_num(ci.rq3$fixed[4, 3], digits = 3)`], $R^2$ =
`r suppressWarnings(apa_num(as.numeric(r.squaredGLMM(rq3.model)[1]), gt1 = FALSE))`. Therefore, there was no effect of the interaction of average fairness with semester differences in predicting reliability. Similarly, average fairness did not predict reliability overall, *b* =
`r apa_num(coef.rq3[3,1], digits = 3)`, 95% CI [`r apa_num(ci.rq3$fixed[3, 1], digits = 3)`,
`r apa_num(ci.rq3$fixed[3, 3], digits = 3)`]. 

```{r figure3, eval = FALSE, message = FALSE, include = TRUE, fig.cap="Example simple slope depiction for low, average, and high fairness scores used to moderate the relationship between semester time and reliability estimates."}

ggplot(save_rel, aes(z_semester_diff, rel)) + 
  geom_point(alpha = .1) + 
  theme_classic() + 
  xlab("Centered Semester Difference") + 
  ylab("Reliability") + 
  geom_abline(intercept = coef.rq3[1,1], slope = coef.rq3[3,1], color= "red", 
                 linetype= "dashed", linewidth = 1.5) + 
    geom_abline(intercept = coef.rq3.low[1,1], slope = coef.rq3.low[3,1], color= "green", 
                 linetype= "dotted", linewidth = 1.5) + 
    geom_abline(intercept = coef.rq3.high[1,1], slope = coef.rq3.high[3,1], color= "blue", 
                 linetype= "dashed", linewidth = 1.5) + 
  geom_vline(xintercept = 0) + 
  theme(axis.line.y = element_blank())

ggsave("output/figure3.png")
```

## RQ 4

```{r rq4-nlme}
# center the variables
save_rel$std_fair_center <- scale(save_rel$std_fair, scale = FALSE)
save_rel$std_fair_low <- save_rel$std_fair_center + sd(save_rel$std_fair_center, na.rm = T)
save_rel$std_fair_high <- save_rel$std_fair_center - sd(save_rel$std_fair_center, na.rm = T)

rq4.model <- lme(rel ~ z_semester_diff*std_fair_center, 
                 data = save_rel,
                 random = list(~1|instructor_code.x), 
                 method = "ML", 
                 na.action = "na.omit",
                 control = lmeControl(opt = "optim"))

coef.rq4 <- summary(rq4.model)$tTable
ci.rq4 <- intervals(rq4.model)

rq4.model.low <- lme(rel ~ z_semester_diff*std_fair_low, 
                 data = save_rel,
                 random = list(~1|instructor_code.x), 
                 method = "ML", 
                 na.action = "na.omit",
                 control = lmeControl(opt = "optim"))
coef.rq4.low <- summary(rq4.model.low)$tTable

rq4.model.high <- lme(rel ~ z_semester_diff*std_fair_high, 
                 data = save_rel,
                 random = list(~1|instructor_code.x), 
                 method = "ML", 
                 na.action = "na.omit",
                 control = lmeControl(opt = "optim"))
coef.rq4.high <- summary(rq4.model.high)$tTable
```

The confidence interval for the interaction of variability of fairness and semester time difference did cross zero, *b* = `r apa_num(coef.rq4[4,1], digits = 3)`, 95% CI
[`r apa_num(ci.rq4$fixed[4, 1], digits = 3)`, `r apa_num(ci.rq4$fixed[4, 3], digits = 3)`],
$R^2$ =
`r suppressWarnings(apa_num(as.numeric(r.squaredGLMM(rq4.model)[1]), gt1 = FALSE))`. The variability of fairness also did not predict reliability overall, *b* =
`r apa_num(coef.rq4[3,1], digits = 3)`, 95% CI [`r apa_num(ci.rq4$fixed[3, 1], digits = 3)`,
`r apa_num(ci.rq4$fixed[3, 3], digits = 3)`]. 

```{r figure4, eval = FALSE, message = FALSE, include = TRUE, fig.cap="Example simple slope depiction for low, average, and high fairness variability used to moderate the relationship between semester time and reliability estimates."}

ggplot(save_rel, aes(z_semester_diff, rel)) + 
  geom_point(alpha = .1) + 
  theme_classic() + 
  xlab("Centered Semester Difference") + 
  ylab("Reliability") + 
  geom_abline(intercept = coef.rq4[1,1], slope = coef.rq4[3,1], color= "red", 
                 linetype= "dashed", linewidth = 1.5) + 
    geom_abline(intercept = coef.rq4.low[1,1], slope = coef.rq4.low[3,1], color= "green", 
                 linetype= "dotted", linewidth = 1.5) + 
    geom_abline(intercept = coef.rq4.high[1,1], slope = coef.rq4.high[3,1], color= "blue", 
                 linetype= "dashed", linewidth = 1.5) + 
  geom_vline(xintercept = 0) + 
  theme(axis.line.y = element_blank())

ggsave("output/figure4.png")
```

# Discussion

- Summarize the results

## What Should I Do with This Information

- Don't expect to be reliable across other classes
- Don't expect to be reliable over long period of time, people change, students change, etc. 

## Strengths

- a crap ton of data
- over a long period of time
- robust results 

## Limitations

- one item versus many
- evaluations don't mean what we want them to mean
- one uni means maybe not generalizable 

## Future Work



\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
