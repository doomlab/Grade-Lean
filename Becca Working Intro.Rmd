---
title: "Notes for PG Manuscript"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).

Student evaluations of professors have been disputed over time with regard to validity and reliability. The impact of student evaluations on professor advancement can be great and can often act as a deciding factor in professor promotion or demotion, along with the access to funding opportunities, coursework choice, and tenureship. There are certain variables researched that result in improving evaluations, such as giving higher grades [@Greenwald1997; @Isely2005; @Krautmann1999]. Further, (Neath 1996) suggests 20 tips in which professors may bolster their evaluations from students that have no relationship with proven instructional methods or further learning retention among the student body. Engaging in these steps to improve evaluations further adds to the argument that student evaluations may not necessarily be measuring whether the instructional methods of professors are sound, rather measuring whether or not the professor is likeable among the students. 

However, some authors [@Centra2003; @Marsh1987; @Marsh2000] have discovered professors are not able to increase their positive evaluations by providing their students with higher grades. We believe this is due to what we consider the effect of "perceived grading". We operationally define perceived grading as the students' perception of the fairness of the grading system in a particular course, and we add to that the students' current gpa while taking a particular course and the expected grade the student intends to achieve in a course during the evaluation process. We believe social psychology theory would support that students with low perceived grading can reduce cognitive dissonance and engage in ego defense by giving low evaluations of professors who give them lower grades [@Maurer2006] resulting in decreased validity and reliability of the construct, professor instruction. We argue that both social psychology theory and the evidence from student evaluations supports that higher perceived grading can lead to better student evaluations of instruction. This theory and evidence from student evaluation leads us to further posit student evaluations of professors as biased methods of data collection and irrelevant to the quality of the instructor and the instructional methods used over the course of a semester. 

Much of the research on student evaluations involves diverse and complex analyses (e.g., @Marsh1987) and lacks social-psychological theoretical guidance on human judgment. To expect that student evaluations would not be influenced by expected grade would contradict a long-standing history of social psychology research on cognitive dissonance, attribution, and ego threat. Failure threatens the ego [@Miller1985; Snyder, Stephan & Rosenfield, 1978] and motivates us to find rationales to defend the ego. Failing students, or those performing below personal expectations, would be expected to defend their ego by attributing low grades to poor teaching or unfair evaluation practices [@Maurer2006]. One common strategy involves diminishing the value of the activity [@Miller1989], which would result in lowered value of a course.

Similarly, Cognitive Dissonance Theory [@Festinger1957] predicts that people who experience poor performance but perceive themselves as competent will experience dissonance, by which they can reduce through negative evaluations of the instruction [@Maurer2006]. Attribution research [@Weiner1992] also supports the argument that among low achievement motivation students, failure is associated with external attributions for cause, and the most plausible external attribution is the quality of instruction and grading practices. Although arguments regarding degree of influence are reasonable, the position that they are not affected is inconsistent with existing and established theory. Thus, it is not surprising that the majority of faculty perceive student evaluations to be biased by perceived grading and course choice [@Marsh1987].

Considerable research has been conducted in support of widely distributed evaluation systems. @Centra2003 reported that in a study of 9,194 class averages using the Student Instructional Support, the relationship between expected grades and global ratings was only .20. He further argued that when variance due to perceived learning outcomes was regressed from the global evaluation, the effect of expected grades was eliminated. However, a student’s best assessment of "perceived learning outcome" is their expected grade, and thus, these should be highly correlated. When perceived learning is regressed from the global evaluations, it is not surprising that suppression effects would eliminate or could even reverse the correlation between expected grade and global evaluation. In general, there are several reasons why the relationship of expected grade to global evaluations is suppressed. For example, faculty ratings are generally very high on average (i.e. quality instructors are hired), which restricts variation; thus, weakening their reliability as a measure of teacher attributes. This restriction in range suppresses correlation.

Perhaps a more significant reason the grade/evaluation effect is often minimized in research is due to the confusion between leniency in perceived grading and expected grade. We describe this concept as leniency because the inflated grades are higher than grade descriptors would imply. The grading system in the university where data were collected describes a C as "Average" and a B as "Above Average". @Isely2005 demonstrated that student evaluations are influenced by the difference between expected grade and the student’s GPA. Expected grade is experienced by the student as leniency to the extent it reflects differences from course expectations based upon course effort and typical grades for the student. The authors recognize that many professors would argue that they give the majority of their students As because they are extraordinary teachers or because they use a mastery system designed to allow all students to receive an A. This ideology reflects an abandonment of grades as a means to differentiate students, a departure from "C = average", and a lack of matching levels of challenge to the achievement potential of students. For our purposes, we refer to it as leniency to the extent that grading is above the university standard of "C = average". 

@Marsh1987 argued that the individual is not the proper unit of analysis because such analyses could suggest false findings related to individual differences in students. Therefore, he argued the use of class as the suggested unit of analysis. We agree, both for his reasoning and because analyses with individual ratings can mask significant relationships as well. Individual differences in expectancy will attenuate the correlation less when class average is used as the unit of analysis. To the extent that the same class average would be expected across all courses, an assumption we will challenge, the class average for expected grade is a good measure of perceived grading as an instructor attribute. Course quality, not individual attributes of students, is what we are attempting to assess when we are using student evaluations of courses. Several studies provide support that when class is the unit of analysis, expected grade is a more significant biasing factor in student evaluations [@Blackhart2006; @DuCette1982; @Ellis2003].

@Blackhart2006 analyzed 167 psychology classes in a multiple regression analysis with class as the unit of analysis and found that the two most significant predictors of instructor ratings were average grade given by the instructor and instructor status (TA or rank of faculty). Because of the limited number of classes, the power of the analysis was limited. However, in addition to the concern regarding the relationship between grades and global course evaluations, it was found that TAs were rated more highly than ranked faculty. This finding raises additional questions on validity. We must either accept that the least trained and qualified teachers are actually better teachers, or we must believe this result suggests that student evaluations have given us false information on the quality of teaching via their perceptions of grading.

@DuCette1982 provided evidence that using course as a unit of analysis increased the correlation between expected grade and other course ratings. Within specific groupings of classes, these correlations ranged from .23 to .53. Two factors limited the level of their relationships. First, the classes used were all upper division courses and graduate courses. Secondly, over 90% of the students in these classes expected an A or a B. Consequently, the correlations between expected grade and global course ratings would be reduced due to the absence of variation in expected grades. Similarly, @Ellis2003 found a correlation of .35 between average course grade and average rating of the instructor in 165 classes during a two-year period. However, these studies did not consider the predictive relationship for instructors across different courses and semesters.

Different disciplines and subject areas have diverse GPA standards, and students have differing grade and workload expectations in different courses as well. For example, an instructor in Anatomy giving a 3.00 GPA might be considered lenient while an Education instructor giving a 3.25 GPA might be considered hard (examples for illustration only). To have a valid measure of workload and leniency factors, correlations should be conducted with varied teachers of the same course. Further, different populations take courses in different disciplines, resulting in potential population differences between anatomy classes and education classes, which could create or mask findings as well. Hence, analysis of these correlations within the same discipline and course was expected to strengthen the relationship between expected grades and quality measures, offering more valid results. 

In most studies of student evaluations, reliability is established through internal consistency reliability. However, this form of reliability is confounded with halo effects (i.e. a cognitive bias that influences ratings based on an overall perception of the person teaching, rather than the individual components of the course), and tells only whether the individual responding to the questions is consistent and reliable. By having many different classes for the same instructor, we can establish the reliability of ratings across the same and different courses during the same and different semesters. Student ratings cannot be considered a valid measure of an instructor’s teaching skills if they are unable to reliably differentiate instructors. 

If ratings are valid measures of instructor attributes, it should be expected that ratings would have some stability across semester and course taught. If variation were due to instructor attributes and not the course they are assigned, we would expect ratings to be most stable across two different courses during the same semester. We would expect these correlations to decline somewhat for the same course in a different semester, since faculty members may improve or decline with experience. But if they are reliable and stable enough to use in making choices about retention, their stability should be demonstrated across different semesters as well. Therefore, we first sought to establish reliability of ratings for the instructors across courses and semesters. 

In the current study, we used data collected over a 20-year period to allow for more powerful analyses, with such analyses occurring within many sections of the same course. After examining reliability, we sought to show that items on instructor evaluations were positively correlated for undergraduate and graduate students, demonstrating that overall course evaluations are related to perceptions of grading and expected grade. We also expected correlations to be substantially higher than those obtained by researchers who used individual students as their unit of analysis. Next, we examined if rating differences across these questions were found between types of instructors compared to full-time faculty, such as teaching-assistants and per-course faculty. The presumption of university hiring requirements that include a terminal degree for regular faculty is that better-trained faculty will be more effective teachers. Therefore, if student evaluations are a valid measure, better-trained full-time faculty should receive higher ratings than per-course instructors and teaching assistants. However, existing literature appears to contradict this expectation [@Blackhart2006]. Given these differences, we proposed and examined a moderated mediation analysis to portray the expected relationship of the variables across instructor type.

— Student evaluations are biased and not a good measure of instruction (need articles to reflect this idea in introduction to support what we say in discussion) 
— Are evaluations reliable? Other studies show they’re biased by gender but maybe also bias by type of instructor and perceived grading
— clear up definition of perceived grading

