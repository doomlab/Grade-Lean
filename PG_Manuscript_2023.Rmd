---
title             : "The Reliability of Instructor Evaluations"
shorttitle        : "RELIABILITY EVALUATIONS"
author: 
  - name          : "Erin M. Buchanan"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "326 Market St., Harrisburg, PA 17010"
    email         : "ebuchanan@harrisburgu.edu"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
      - "Analysis"
  - name          : "Jacob Miranda"
    affiliation   : "2"
    role:
      - "Writing - Original Draft Preparation"
  - name          : "Christian Stephens"
    affiliation   : "2"
    role:
      - "Writing - Original Draft Preparation"
affiliation:
  - id            : "1"
    institution   : "Harrisburg University of Science and Technology"
  - id            : "2"
    institution   : "University of Alabama"
authornote: |
  TBA
abstract: |
  TBA
  
keywords          : "keywords"
floatsintext      : no
linenumbers       : yes
draft             : no
mask              : no
figurelist        : no
tablelist         : no
footnotelist      : no
classoption       : "man"
output            : papaja::apa6_docx
editor_options: 
  chunk_output_type: inline
bibliography: references.bib
---

```{r setup, include = FALSE}
library("papaja")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(89432289)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, echo = FALSE, include = FALSE, cache = TRUE, cache.lazy = FALSE)
library(rio)
library(dplyr)
library(nlme)
library(effectsize)
library(ggplot2)
library(ppcor)
library(MuMIn)
```

In the United States, college and university professors are evaluated to
varying degrees on research productivity, service, and teaching
effectiveness. These dimensions are often used for high-stakes
administration decisions [e.g., hiring, retention, promotion, pay, and
tenure, @freishtat2014; @hornstein2017; @spooren2013] !!stoebe, 2020!!.
Depending on the institution, a major failure of one these areas could
jeopardize a professors' position within the department; thus,
evaluating research, service, and teaching is of the utmost importance.
Focusing on evaluating educators on teaching effectiveness, however, is
both difficult and costly. Indeed, the vast majority of the 9,000
professors polled by the American Association of University Professors
shared that teaching needs to be taken as seriously as research and
service (!!Flaherty, 2015!!). As students consider rising tutition
costs, perceived quality education can improve student engagement and
retention.

Teaching effectiveness can be defined as the degree to which student
achievement is facilitated [i.e., how much have students learned in a
particular course; @cohen1981]. Generally, the assessment of teaching
effectiveness comes from students and their evaluations which may focus
on the instructor or the course specifically [e.g., "Student Opinion of
Instruction", "Student Evaluations of Teaching", "Students Opinion of
Teaching Effectiveness", "Students Evaluation of Faculty", "Overall
Course Ratings", "Instruction Rating"; @cohen1981]. !!Flaherty, 2020!!
Often these are described as "quality" of an individual course
[@marsh2007; @gillmore1978]. Teaching effectiveness measures are
designed to tap into factors of teaching, such as communication,
organization, instructor behavior, grading, and more [@hattie1996].
Given teaching evaluations use in administrative decisions, both
reliability and validity should be demonstrated for the measurement to
have utility. Therefore, the natural question arises: are students'
evaluation of the course and/or instructor reliable and valid measures
of teaching effectiveness?

## Validity 

@sheehan1975's review of the literature nearly 50 years ago indicated
multiple factors of bias that likely exist within student evaluations:
1) student demographics: gender, class, age, previous achievement, 2)
class type: subject matter, size, degree requirements, and 3)
instructor: gender, rank, gender-match to student. Even now, these
concerns remain [@boring2016; @uttl2017; @hornstein2017] !! dunn et al.,
2016!!. @cohen1981's early work on the relationship between student
achievement and instruction rating indicated a potential moderate
relationship; however, recent meta-analyses demonstrate that student
evaluations of teaching are likely unrelated to learning [@uttl2017].
@boring2016 estimate that the bias in student evaluations are unable to
be fixed due to the complex interaction of factors within evaluations.

Systemic reviews and recent studies underscore that sexism (e.g.,
MacNell et al., 2014; Mitchell & Martin, 2018), racism (e.g. Smith &
Hawkins, 2011), and general bias pervades students' evaluations of
traditional courses and possibly exist for online ones as well (e.g.,
Heffernan, 2021; Rovai et al., 2006; Sullivan et al., 2013; Zheng et
al., 2023). Individual factors may also yield some influence, including
instructors' cultural background (e.g., Fan et al., 2019),
attractiveness (e.g., Felton et al., 2008; Wright, 2008), position
ranking (e.g., Johnson et al., 2013), and students' expected grade from
the course (e.g., Chen et al., 2017; Crumbly et al., 2001; Marks, 2000).
Others suggest biasing factors of students' ratings include the volume
of the instructor's voice and how legible their instructor's writing is
(Becker et al., 2012). Stroebe (2018) underscores the possible danger of
an incentive system that is tied to student ratings; instructors may be
then incentivized to be a less effective teacher (e.g., grade leniently,
choose to teach courses based off student interest) rather than
challenge students critically.

One of the most commonly proposed solutions is to use multiple
evaluations of teaching effectiveness (e.g., subject-matter sit-ins on
lecture, peer reviews of course curriculum, Benton & Young, 2018; Berk,
2018; Esarey & Valdes, 2020; Kornell & Hausman, 2016). However, the cost
of implementing a more accurate multi-pronged approach may be more than
universities can afford, especially given tight budgets and current
instructor expectations. The current zeitgeist is often to continue
using student evaluations of teaching as the most affordable solution in
terms of both time and money. Students' ratings may show some utility at
indicating to other students which classes to pursue and with whom
(e.g., Stankiewicz, 2015), and unfortunately, even if instructors
believe such ratings to be an inappropriate, it may influence their
self-efficacy as an educator regardless (Boswell, 2016). While student
evaluations are often considered non-valid measurements of teaching
effectiveness, others argue that calls for the complete removal
students' voices from the process is potentially the wrong course of
action (Benton and Ryalls (2016)).

## Perceived fairness 

Our study focused on potential sources of validity bias using ratings of
grading within the course (which will be called perceived fairness).
Extant research tends to confirm that instructor evaluations are
influenced by students' grades, possibly pressuring some instructors
into reducing the rigor of their course for the sake of attaining higher
evaluation ratings (Greenwald & Gillmore, 1997; Marks, 2000). However,
as pointed out by Wright (2008), students' expectations of their final
grades may not affect ratings nearly as much as their perceived fairness
of the grading process. Professors who are consistent, representative,
accurate, unbiased, and correctable in their grading may receive high
evaluation ratings regardless of how much a student learns or what
his/her final grade turns out to be (Horan et al., 2010; Leventhal,
1980). Thus, grades may predict evaluation ratings only so much as
students perceive their grade and the processes by which they were
determined as fair (Tata, 1999).

Additionally, the different facets leading into a final grade's
calculation may play on each other as students consider fairness in
their evaluations. For example, Tripp and colleagues (2019) found that
students' perceived fairness of their instructors' grading processes
affected their perceived fairness of their assigned grade, which then
translated to their evaluation ratings of teacher effectiveness.
Perceived fairness of the course workload and difficulty may also be
inversely related to perceived fairness of the grading process as a
challenging professor may be thought of as less fair (Marks, 2000).
Access to grading criteria, frequency of feedback, and proactive
instruction are other aspects of grading known to explicitly affect
perceived fairness (Pepper & Pathak, 2008); in turn, the fairness of
these aspects must be factored in as well. Taken together, students'
perceived fairness of grading may be more akin to comprehensive
assessments of the instructor rather than face-value judgments of their
grade.

## Reliability 

Past investigations utilizing large samples concluded student ratings
are reliable and stable (e.g., Arubayi, 1987; Marsh & Roche, 1997). More
recently, a review found that students' ratings within the same class
tend to be internally consistent when teaching effectiveness was
assessed through several items, reliable across students within the same
class, and reliable across the same instructor across multiple courses
(Benton & Cashin, 2014). Students who rated a retrospectively rated a
course one to three years after the course showed high correlations with
their previous course ratings [@overall1980]. Results from studies that
tease apart variance in ratings due to instructor, course, and student
factors indicate that each is an essential source of variance, which can
influence the reliability of instruction evaluation [@feistauer2017]. In
general, research appears to support the reliability of student
evaluations of teaching, yet, only a few studies have examined this
reliability across instructor, course, and time. Research into teaching
effectiveness appears to suggest that instructors have stable
evaluations over time [@marsh2007], and our study extends this work to
examine reliability patterns over 30 years of evaluations.

## The current study 

The current study is similar in scope to recent work (e.g., Boring et
al., 2016; Fan et al., 2019) in its calibration of teacher evaluations
collected over an extensive period. Boring and colleagues' (2016)
investigation on both French instructors and U.S. teaching assistants'
gender ranged across five years; similarly, Fan and peers (2019)
investigated the topic across seven. Their utilization of multi-sections
has been described as the gold standard for researching students'
ratings. Thus, we aimed to follow their lead by analyzing the
reliability of students' ratings provided the same or different
instructor, course type, and/or semester in addition to testing
reliability over more than 30 years of data. We examined the impact of a
potential validity variable on the reliability of ratings using
perceived fairness of grading. Therefore, we sought to explore the
following research questions:

Exploratory Research Questions:

1)  What is the reliability of instructor evaluations?
2)  Are instructor evaluations reliable across time?
3)  Is the average level of perceived fairness of the grading in the
    course a moderator of reliability in instructor evaluations?
4)  Does the average variability in instructor fairness rating moderate
    reliability of instructor evaluations?

The following was pre-registered as a secondary data analysis at:
<https://osf.io/czb4f>. The manuscript, code, and data can be found on
our Open Science Framework page at: <https://osf.io/k7zh2/> or GitHub:
<https://github.com/doomlab/Grade-Lean>. This manuscript was written
with the *R* packages *papaja* [@aust2022], *rio* [@becker2021], *dplyr*
[@R-dplyr], *nlme* [@pinheiro2017], *ggplot2* [@R-ggplot2], *MuMIn*
[@barton2020], *ppcor* [@kim2015], and *effectsize*
[@ben-shachar(mattansb)2023].



# Method

## Data Source

```{r import-data}
# please note the data is deidentified but timing and course number is accurate
DF <- import("data/final_evals.csv")

# we will only use courses with at least 15 ratings ... look at the paper that may be reliable (2013)
DF_sample <- DF %>% 
  filter(People >= 15)

# for this pre-reg we will randomly sample 20 percent of instructors 
# random_sample <- sample(unique(DF_sample$instructor_code), 
#                         size = round(length(unique(DF_sample$instructor_code)) * .30),
#                         replace = FALSE)
# 
# DF_sample <- DF_sample %>% 
#   filter(instructor_code %in% random_sample)
```

The archival study was conducted using data from the psychology
department at a large Midwestern public university. We used data from
`r nrow(DF_sample[ DF_sample$course_level == "undergraduate" , ])`
undergraduate, `r nrow(DF_sample[ DF_sample$course_level == "mixed", ])`
mixed-level undergraduate, and
`r nrow(DF_sample[ DF_sample$course_level == "masters" , ])` graduate
psychology classes taught from 1987 to 2018 that were evaluated by
students using the same 15-item instrument. Faculty followed set
procedures in distributing scan forms no more than two weeks before the
conclusion of the semester. A student was assigned to collect the forms
and deliver them to the departmental secretary. The instructor was
required to leave the room while students completed the forms. In the
last several years of evaluations, online versions of these forms were
used with faculty encouraged to give students time to complete them in
class while they were outside the classroom.

The questionnaire given to students can be found at
<https://osf.io/4sphx>. These items were presented with a five-point
scale from 1 (*strongly disagree*) to 5 (*strongly agree*). For this
study, the overall instructor evaluation question was "The overall
quality of this course was among the top 20% of those I have taken.".
For fairness, we used the question of "The instructor used fair and
appropriate methods in the determination of grades.". The ratings were
averaged for each course, and the sample size for each rating was
included.

## Planned Analyses

The evaluations were filtered for those with at least fifteen student
ratings for the course [@rantanen2012]. We performed a robustness check
for the first research question on the data when the sample size is at
least *n* = 10 up to *n* = 14 (i.e., on all evaluations with at least 10
ratings, then at least 11 ratings, etc.) to determine if the reliability
estimates are stable at lower sample sizes. We first screened the
dataset (two evaluation questions, sample size for course) for accuracy
errors, linearity, normality, and homoscedasticity. The data is assumed
to not have traditional "outliers", as these evaluations represent true
averages from student evaluations. If the linearity assumption fails, we
considered potential nonparametric models to address non-linearity.
Deviations from normality were noted as the large sample size should
provide robustness for any violations of normality. If data appears to
be heteroscedastic, we used bootstrapping to provide estimates and
confidence intervals.

This data was considered structured by instructor; therefore, all
analyses below were coded in *R* using the *nlme* package
[@pinheiro2017] to control for correlated error of instructor as a
random intercept in a multilevel model. Multilevel models allow for
analysis of repeated measures data without collapsing by participant
[i.e., each instructor/semester/course combination can be kept separate
without averaging over these measurements; @gelman2006]. Random
intercept models are regression models on repeated data that structure
the data by a specified variable, which was instructor in this analysis.
Therefore, each instructor's average rating score was allowed to vary
within the analysis, as ratings would be expected to be different from
instructor to instructor. In each of the analyses described below, the
number of students providing ratings for the course was included as a
control variable to even out differences in course size as an influence
in the results. However, this variable was excluded if the models did
not converge. The dependent variable and predictors varied based on the
research question, and these are described with each analysis below.

### RQ 1

In this research question, we examined the reliability of instructor
evaluations on the overall rating and separately on the fairness rating.
We calculated eight types of reliability using course (same or
different) by instructor (same or different) by semester (same or
different). The dependent variable was the first question average with a
predictor of the comparison question average, and both sample sizes
(first sample size, comparison sample size). Instructor code was used as
the random intercept for both ratings (i.e., two instructor random
intercepts, first and comparison). The value of interest was the
standardized regression coefficient for the fixed effect of question
from this model. Given that the large sample size will likely produce
"significant" *p*-values, we used the 95% CI to determine which
reliability values were larger than zero and to compare reliability
estimates to each other.

### RQ 2

We used the reliability for the same instructor and course calculated as
described in RQ1 at each time point difference between semesters. For
example, the same semester would create a time difference of 0. The next
semester (Spring to Summer, Summer to Fall, Fall to Spring) would create
a time difference of 1. We used the time difference as a fixed effect to
predict reliability for the overall question only with a random
intercept of instructor. We used the coefficient of time difference and
its confidence interval to determine if there was a linear change over
time. Finally, we plotted the changes over time to examine if this
effect was non-linear in nature and discuss implications of the graph.

### RQ 3

Using the reliability estimates from RQ 2, we then added the average
rating for the fairness question as the moderator with time to predict
reliability. Fairness was calculated as the average of the fairness
question for all courses involved in the reliability calculation for
that instructor and time difference. Therefore, this rating represented
the average perceived fairness of grading at the time of ratings. If
this interaction effect's coefficient does not include zero, we
performed a simple slopes analysis to examine the effects of instructors
who were rated at average fairness, one standard deviation below
average, and one standard deviation above average [@cohen2003].

### RQ 4

Finally, we examined the average standard deviation of fairness ratings
as a moderator of with time to predict reliability. This variable
represented the variability in perceived fairness in grading from
student evaluations, where small numbers indicated relative agreement on
the rating of fairness and larger values indicated a wide range of
fairness ratings. The variability in fairness ratings was calculated in
the same way as the mean fairness, which was only for the instructor and
semester time difference evaluations that were used to calculate the
reliability estimate. This research question was assessed the same way
as research question three.

# Results

## Data Screening

```{r datascreening, include = FALSE}
random <- rchisq(nrow(DF_sample), 7)
output <- lm(random ~ ., data = DF_sample %>% dplyr::select(People, Q1AVG, Q4AVG))
standardized <- rstudent(output)
fitvalues <- scale(output$fitted.values)

#linear
{qqnorm(standardized)
abline(0,1)}

#multivariate normality
hist(standardized, breaks=15)

#homogeneity and homoscedasticity
{plot(fitvalues, standardized) 
abline(0,0)
abline(v = 0)}
```

The overall dataset was screened for normality, linearity, homogeneity,
and homoscedasticity using procedures from @tabachnick2019. Data
generally met assumptions with a slight skew and some heterogeneity. The
complete anonymized dataset and other information can be found online at
<https://osf.io/k7zh2>. This page also includes the manuscript written
inline with the statistical analysis with the *papaja* package
[@aust2022] for interested researchers/reviewers who wish to recreate
these analyses. The bootstrapped versions of analyses and robustness
analysis can be found online on our OSF page with a summary of results.
We originally planned to bootstrap all analyses; however, the compute
time for research question 1 was extremely long due to the size and
complexity of the multilevel models, and therefore, we did not bootstrap
that research question.

## Descriptive Statistics

`r nrow(DF_sample)` evaluations included at least 15 student evaluations
for analysis. Table \@ref(tab:table1) portrays the descriptive
statistics for each course level including the total number of
evaluations, unique instructors, unique course numbers, and average
scores for the two rating items. Students additionally projected their
course grade for each class (*A* = 5, *B* = 4, *C* = 3, *D* = 2, *F* =
1), and the average for this item is included for reference. Overall,
`r length(unique(DF_sample$instructor_code))` unique instructors and
`r length(unique(DF_sample$new_course))` unique courses were included in
the analyses below across `r max(DF_sample$semester_count) + 1`
semesters.

```{r table1, results = 'asis', include = TRUE}
DF_summary <- DF_sample %>% 
  group_by(course_level) %>% 
  summarize(totaln = n(), 
            num_instruct = length(unique(instructor_code)),
            num_courses = length(unique(new_course)),
            avg_people = apa_num(mean(People, na.rm = T)), 
            avgq1 = apa_num(mean(Q1AVG, na.rm = T)), 
            avgsd1 = apa_num(sd(Q1AVG, na.rm = T)), 
            avgq4 = apa_num(mean(Q4AVG, na.rm = T)),
            avgsd4 = apa_num(sd(Q4AVG, na.rm = T)),
            avgq15 = apa_num(mean(Q15AVG, na.rm = T)), 
            avgsd15 = apa_num(sd(Q15AVG, na.rm = T))) %>% 
  arrange(desc(course_level))

temp <- DF_summary %>% 
  t() %>% 
  as.data.frame() %>% 
  slice(-1)

temp$stat <- c("N Total", "N Instructors", "N Courses",
               "Average N Ratings", "Average Overall", 
               "SD Overall", "Average Fairness", 
               "SD Fairness", "Average Grade", "SD Grade")
apa_table(temp[ , c(4, 1:3)], 
          caption = "Descriptive Statistics of Included Courses", 
            row.names = FALSE, 
            col.names = c("Statistic", "Undergraduate", "Mixed", "Master's"))
```

## RQ 1

```{r rq1-nlme}
# let's see if twitter can help here - yes! @smartin2018 for the win!

# create unique id
DF_sample$evalid <- 1:nrow(DF_sample)

# create every pairwise combination 
combns <- t(combn(DF_sample$evalid,2))

# create a dataframe to hold that information, remove other frame for the love of space
longrel <- data.frame(evalid=combns[,1],evalid2=combns[,2]); rm(combns)

# add person 1 information back to dataframe
templong <- merge(longrel, DF_sample, by = "evalid")
colnames(templong)[1:2] <- c("evalid1", "evalid")

# add person 2 information back to the dataframe, remove other frame for the love of space
longrel <- merge(templong, DF_sample, by = "evalid")
rm(templong)

# create factors
longrel$instructormatch <- as.numeric(longrel$instructor_code.x == longrel$instructor_code.y)
longrel$semestermatch <- as.numeric(longrel$semester_count.x == longrel$semester_count.y)
longrel$coursematch <- as.numeric(longrel$new_course.x == longrel$new_course.y)

#look at the contingency table
#table(longrel$instructormatch, longrel$semestermatch, longrel$coursematch)

# dataframe to put stuff in ----
betarel <- matrix(NA, nrow = 2*2*2*2, ncol = 4+3)

# going to let these overwrite each other so memory isn't bonkers
# semester, course, instructor
## same same same ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y),
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[1 , ] <- c(1,1,1,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[2 , ] <- c(1,1,1,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

## same same different ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[3 , ] <- c(1,1,0,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[4 , ] <- c(1,1,0,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

## same different same ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[5 , ] <- c(1,0,1,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[6 , ] <- c(1,0,1,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

## same different different ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[7 , ] <- c(1,0,0,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 1 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[8 , ] <- c(1,0,0,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

## different different different ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[9 , ] <- c(0,0,0,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[10 , ] <- c(0,0,0,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

## different same different ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[11 , ] <- c(0,1,0,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 0, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[12 , ] <- c(0,1,0,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

## different different same ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[13 , ] <- c(0,0,1,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 0 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[14 , ] <- c(0,0,1,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

## different same same ----
relq1 <- lme(Q1AVG.y ~ Q1AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq1)
betarel[15 , ] <- c(0,1,1,"Overall", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq1)

relq4 <- lme(Q4AVG.y ~ Q4AVG.x + People.x + People.y, 
           data = longrel[ longrel$semestermatch == 0 & 
                             longrel$coursematch == 1 & 
                             longrel$instructormatch == 1, ], 
           method = "ML", 
           na.action = "na.omit",
           random = list(~1|instructor_code.x, ~1|instructor_code.y), 
           control = lmeControl(opt = "optim"))
temp <- standardize_parameters(relq4)
betarel[16 , ] <- c(0,1,1,"Fair", temp$Std_Coefficient[2], temp$CI_low[2], temp$CI_high[2])
rm(relq4)

betarel <- as.data.frame(betarel)
colnames(betarel) <- c("Semester", "Course", "Instructor", "Question",
                      "b", "CI_Low", "CI_High")
```

Each individual evaluation was compared to every other evaluation
resulting in `r nrow(longrel)` total comparisons. Eight combinations of
ratings were examined using instructor (same, different), course (same,
different), and semester (same, different) on both the overall and
fairness evaluation ratings separately. One of the individual ratings
was used to predict the comparison rating (i.e., question 1 was used to
predict a comparison question 1 for the same instructor, different
instructor, same semester, different semester, etc.), and the number of
ratings (i.e., rating sample size) per question were used as
fixed-effects covariates. The instructor(s) were used as a random
intercept to control for correlated error and overall average rating per
instructor. The effects were then standardized using the *parameters*
package [@lüdecke2023]. The data was sorted by year and semester such
that "predictor" was always an earlier semester predicting a later
semester's scores, except in cases of the the same semester comparisons.
Therefore, positive standardized scores indicate that scores tend to go
up over time, while negative scores indicate that scores tend to go down
over time.

As shown in \@ref(fig:figure1), reliability was highest when calculated
on the same instructor in the same semester and within the same course
for both overall rating and fairness. This reliability was followed by
the same instructor, same semester, and different courses. Next, the
reliability for same instructor, same course, and different semesters
was greater than zero and usually overlapped in confidence interval with
same instructor, same semester, and different courses. Interestingly,
the same instructor with different courses and semesters showed a
non-zero negative relationship, indicating that ratings generally were
lower for later semesters in different courses.

For different instructors, we found positive non-zero reliablities when
they were at least calculated on the same semester or course. These
values were very close to zero, generally in the .01 to .05 range. Last,
the reliabilities that were calculated on different courses, semesters,
and instructors include zero in their confidence intervals. Exact values
can be found in the online supplemental document with the robustness
analysis in csv format. Robustness analyses revealed the same pattern
and strength of results for evaluation reliabilities when sample size
for evaluations was considered at *n* = 10, 11, 12, 13, and 14.

```{r figure1, include = TRUE, fig.cap= "Reliability estimates for instructor, course, and semester combinations."}
betarel$b <- as.numeric(betarel$b)
betarel$CI_Low <- as.numeric(betarel$CI_Low)
betarel$CI_High <- as.numeric(betarel$CI_High)
betarel$Semester_nice <- factor(betarel$Semester, 
                           levels = c(0,1),
                           labels = c("Different", "Same"))
betarel$Instructor_nice <- factor(betarel$Instructor, 
                           levels = c(0,1),
                           labels = c("Different Instructor", "Same Instructor"))
betarel$Course_nice <- factor(betarel$Course, 
                           levels = c(0,1),
                           labels = c("Different Course", "Same Course"))

write.csv(betarel, "output/beta_values_rq1.csv", row.names = F)

ggplot(betarel, aes(Semester_nice, b, color = Course_nice)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = CI_Low, ymax = CI_High), width = 0.2) + 
  theme_classic() + 
  facet_grid(Question~Instructor_nice) + 
  ylab("Reliability") + 
  xlab("Semester Match") + 
  scale_color_discrete(name = "Course Match")

ggsave("output/figure1.png")
```

## RQ 2

```{r rq2-setup, warning = FALSE}
# filter to only matches
long_match <- longrel %>% 
  filter(coursematch == 1) %>% 
  filter(instructormatch == 1) %>% 
  dplyr::select(instructor_code.x, semester_count.x, People.x, Q1AVG.x, Q4AVG.x, 
         instructor_code.y, semester_count.y, People.y, Q1AVG.y, Q4AVG.y) %>% 
  mutate(semester_diff = round(semester_count.y - semester_count.x))

# figure out the number each person has and need at least 10
long_summary <- long_match %>% 
  group_by(instructor_code.x, semester_diff) %>% 
  summarize(total_n = n(), .groups = "keep", 
            avg_fair = mean(c(Q4AVG.x, Q4AVG.y), na.rm = T), 
            std_fair = sd(c(Q4AVG.x, Q4AVG.y), na.rm = T)) %>% 
  filter(total_n >= 10) %>% 
  mutate(code = paste0(instructor_code.x, semester_diff))

long_match$code <- paste0(long_match$instructor_code.x, long_match$semester_diff)

save_rel <- data.frame(
  code = 1:nrow(long_summary), 
  rel = 1:nrow(long_summary)
)

for (i in 1:nrow(long_summary)){
  temp <- long_match %>% 
    filter(code %in% long_summary$code[i]) %>% 
    dplyr::select(Q1AVG.x, Q1AVG.y, People.x, People.y) %>% 
    pcor()
  save_rel$code[i] <- long_summary$code[i]
  save_rel$rel[i] <- temp$estimate[2, 1]
}

save_rel <- save_rel %>% 
  left_join(long_summary %>% dplyr::select(code, instructor_code.x, semester_diff, avg_fair, std_fair) %>% unique(), by = "code")
```

The paired evaluations were then filtered to only examine course and
instructor matches to explore the relation of reliability across time.
Reliability was calculated by calculating the partial correlation
between the overall rating for the course first evaluation and the
overall rating for the course second evaluation, controlling for the
number of ratings within those average scores. This reliability was
calculated separately for each instructor and semester difference (i.e.,
the time between evaluations, 0 means same semester, 1 means the next
semester, 2 means two semesters later, etc.). The ratings were filtered
so that at least 10 pairs of ratings were present for each instructor
and semester difference combination [@weaver2014]. Of
`r nrow(long_match)` possible matched instructor and course pairings,
`r nrow(long_match %>% filter(code %in% long_summary$code))` included at
least 10 pairings, which was `r nrow(long_summary)` total instructor and
semester combinations.

```{r rq2-nlme}
rq2.model <- lme(rel ~ semester_diff, 
                 data = save_rel,
                 random = list(~1|instructor_code.x), 
                 method = "ML", 
                 na.action = "na.omit",
                 control = lmeControl(opt = "optim"))

coef.rq2 <- summary(rq2.model)$tTable
ci.rq2 <- intervals(rq2.model)
```

The confidence interval for the effect of semester difference predicting
reliability did not cross zero, *b* =
`r apa_num(coef.rq2[2 , 1], digits = 3)`, 95% CI
[`r apa_num(ci.rq2$fixed[2,1], digits = 3)`,
`r apa_num(ci.rq2$fixed[2,3], digits = 3)`], $R^2$ =
`r suppressWarnings(apa_num(as.numeric(r.squaredGLMM(rq2.model)[1]), gt1 = FALSE))`.
The coefficient, while small, represents a small effect of time on the
reliability of instructor ratings. As shown in \@ref(fig:figure2),
reliability appears to decrease across time.

```{r figure2, include = TRUE, fig.cap = "Reliability estimates for same instructor and course across time.", message = F}
ggplot(save_rel, aes(semester_diff, rel)) + 
  geom_point(alpha = .5) + 
  theme_classic() + 
  xlab("Semester Difference") + 
  ylab("Reliability") + 
  geom_smooth(method = "lm", color = "black") 

ggsave("output/figure2.png")
```

## RQ 3

```{r rq3-nlme}
# center the variables
save_rel$avg_fair_center <- scale(save_rel$avg_fair, scale = FALSE)
save_rel$avg_fair_low <- save_rel$avg_fair_center + sd(save_rel$avg_fair_center, na.rm = T)
save_rel$avg_fair_high <- save_rel$avg_fair_center - sd(save_rel$avg_fair_center, na.rm = T)
save_rel$z_semester_diff <- scale(save_rel$semester_diff, scale = FALSE)

rq3.model <- lme(rel ~ z_semester_diff*avg_fair_center, 
                 data = save_rel,
                 random = list(~1|instructor_code.x), 
                 method = "ML", 
                 na.action = "na.omit",
                 control = lmeControl(opt = "optim"))

coef.rq3 <- summary(rq3.model)$tTable
ci.rq3 <- intervals(rq3.model)

rq3.model.low <- lme(rel ~ z_semester_diff*avg_fair_low, 
                 data = save_rel,
                 random = list(~1|instructor_code.x), 
                 method = "ML", 
                 na.action = "na.omit",
                 control = lmeControl(opt = "optim"))
coef.rq3.low <- summary(rq3.model.low)$tTable

rq3.model.high <- lme(rel ~ z_semester_diff*avg_fair_high, 
                 data = save_rel,
                 random = list(~1|instructor_code.x), 
                 method = "ML", 
                 na.action = "na.omit",
                 control = lmeControl(opt = "optim"))
coef.rq3.high <- summary(rq3.model.high)$tTable
```

The confidence interval for the interaction of semester time difference
and average fairness did cross zero, *b* =
`r apa_num(coef.rq3[4,1], digits = 3)`, 95% CI
[`r apa_num(ci.rq3$fixed[4, 1], digits = 3)`,
`r apa_num(ci.rq3$fixed[4, 3], digits = 3)`], $R^2$ =
`r suppressWarnings(apa_num(as.numeric(r.squaredGLMM(rq3.model)[1]), gt1 = FALSE))`.
Therefore, there was no effect of the interaction of average fairness
with semester differences in predicting reliability. Similarly, average
fairness did not predict reliability overall, *b* =
`r apa_num(coef.rq3[3,1], digits = 3)`, 95% CI
[`r apa_num(ci.rq3$fixed[3, 1], digits = 3)`,
`r apa_num(ci.rq3$fixed[3, 3], digits = 3)`].

```{r figure3, eval = FALSE, message = FALSE, include = TRUE, fig.cap="Example simple slope depiction for low, average, and high fairness scores used to moderate the relationship between semester time and reliability estimates."}

ggplot(save_rel, aes(z_semester_diff, rel)) + 
  geom_point(alpha = .1) + 
  theme_classic() + 
  xlab("Centered Semester Difference") + 
  ylab("Reliability") + 
  geom_abline(intercept = coef.rq3[1,1], slope = coef.rq3[3,1], color= "red", 
                 linetype= "dashed", linewidth = 1.5) + 
    geom_abline(intercept = coef.rq3.low[1,1], slope = coef.rq3.low[3,1], color= "green", 
                 linetype= "dotted", linewidth = 1.5) + 
    geom_abline(intercept = coef.rq3.high[1,1], slope = coef.rq3.high[3,1], color= "blue", 
                 linetype= "dashed", linewidth = 1.5) + 
  geom_vline(xintercept = 0) + 
  theme(axis.line.y = element_blank())

ggsave("output/figure3.png")
```

## RQ 4

```{r rq4-nlme}
# center the variables
save_rel$std_fair_center <- scale(save_rel$std_fair, scale = FALSE)
save_rel$std_fair_low <- save_rel$std_fair_center + sd(save_rel$std_fair_center, na.rm = T)
save_rel$std_fair_high <- save_rel$std_fair_center - sd(save_rel$std_fair_center, na.rm = T)

rq4.model <- lme(rel ~ z_semester_diff*std_fair_center, 
                 data = save_rel,
                 random = list(~1|instructor_code.x), 
                 method = "ML", 
                 na.action = "na.omit",
                 control = lmeControl(opt = "optim"))

coef.rq4 <- summary(rq4.model)$tTable
ci.rq4 <- intervals(rq4.model)

rq4.model.low <- lme(rel ~ z_semester_diff*std_fair_low, 
                 data = save_rel,
                 random = list(~1|instructor_code.x), 
                 method = "ML", 
                 na.action = "na.omit",
                 control = lmeControl(opt = "optim"))
coef.rq4.low <- summary(rq4.model.low)$tTable

rq4.model.high <- lme(rel ~ z_semester_diff*std_fair_high, 
                 data = save_rel,
                 random = list(~1|instructor_code.x), 
                 method = "ML", 
                 na.action = "na.omit",
                 control = lmeControl(opt = "optim"))
coef.rq4.high <- summary(rq4.model.high)$tTable
```

The confidence interval for the interaction of variability of fairness
and semester time difference did cross zero, *b* =
`r apa_num(coef.rq4[4,1], digits = 3)`, 95% CI
[`r apa_num(ci.rq4$fixed[4, 1], digits = 3)`,
`r apa_num(ci.rq4$fixed[4, 3], digits = 3)`], $R^2$ =
`r suppressWarnings(apa_num(as.numeric(r.squaredGLMM(rq4.model)[1]), gt1 = FALSE))`.
The variability of fairness also did not predict reliability overall,
*b* = `r apa_num(coef.rq4[3,1], digits = 3)`, 95% CI
[`r apa_num(ci.rq4$fixed[3, 1], digits = 3)`,
`r apa_num(ci.rq4$fixed[3, 3], digits = 3)`].

```{r figure4, eval = FALSE, message = FALSE, include = TRUE, fig.cap="Example simple slope depiction for low, average, and high fairness variability used to moderate the relationship between semester time and reliability estimates."}

ggplot(save_rel, aes(z_semester_diff, rel)) + 
  geom_point(alpha = .1) + 
  theme_classic() + 
  xlab("Centered Semester Difference") + 
  ylab("Reliability") + 
  geom_abline(intercept = coef.rq4[1,1], slope = coef.rq4[3,1], color= "red", 
                 linetype= "dashed", linewidth = 1.5) + 
    geom_abline(intercept = coef.rq4.low[1,1], slope = coef.rq4.low[3,1], color= "green", 
                 linetype= "dotted", linewidth = 1.5) + 
    geom_abline(intercept = coef.rq4.high[1,1], slope = coef.rq4.high[3,1], color= "blue", 
                 linetype= "dashed", linewidth = 1.5) + 
  geom_vline(xintercept = 0) + 
  theme(axis.line.y = element_blank())

ggsave("output/figure4.png")
```

# Discussion

-   Summarize the results

## What Should I Do with This Information

-   Don't expect to be reliable across other classes
-   Don't expect to be reliable over long period of time, people change,
    students change, etc.

## Strengths

-   a crap ton of data
-   over a long period of time
-   robust results

## Limitations

-   one item versus many
-   evaluations don't mean what we want them to mean
-   one uni means maybe not generalizable

## Future Work

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
